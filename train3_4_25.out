Total train examples: 3000
Loss at batch 10 : 0.19770961999893188
Loss at batch 20 : 0.1802397221326828
Loss at batch 30 : 0.1381036639213562
Loss at batch 40 : 0.13917326927185059
Loss at batch 50 : 0.1584797203540802
Loss at batch 60 : 0.18058577179908752
Loss at batch 70 : 0.16529791057109833
Loss at batch 80 : 0.1377476304769516
Loss at batch 90 : 0.15215803682804108
Loss at batch 100 : 0.1419297754764557
Loss at batch 110 : 0.14290989935398102
Loss at batch 120 : 0.13362355530261993
Loss at batch 130 : 0.14090923964977264
Loss at batch 140 : 0.14278581738471985
Loss at batch 150 : 0.14082615077495575
Loss at batch 160 : 0.1270368993282318
Loss at batch 170 : 0.12888602912425995
Loss at batch 180 : 0.12426932901144028
Loss at batch 190 : 0.13542018830776215
Loss at batch 200 : 0.12418653070926666
Loss at batch 210 : 0.14814256131649017
Loss at batch 220 : 0.10292890667915344
Loss at batch 230 : 0.13236846029758453
Loss at batch 240 : 0.11680106818675995
Loss at batch 250 : 0.14116498827934265
Loss at batch 260 : 0.133208230137825
Loss at batch 270 : 0.11178494244813919
Loss at batch 280 : 0.10680271685123444
Loss at batch 290 : 0.1081409901380539
Loss at batch 300 : 0.08715168386697769
Loss at batch 310 : 0.11130071431398392
Loss at batch 320 : 0.0942537933588028
Loss at batch 330 : 0.10473868995904922
Loss at batch 340 : 0.11338231712579727
Loss at batch 350 : 0.10031429678201675
Loss at batch 360 : 0.11980480700731277
Loss at batch 370 : 0.1195029616355896
epoch0 finished!
Loss at batch 10 : 0.11911015957593918
Loss at batch 20 : 0.13246138393878937
Loss at batch 30 : 0.11839073151350021
Loss at batch 40 : 0.1062697172164917
Loss at batch 50 : 0.12226459383964539
Loss at batch 60 : 0.11253280937671661
Loss at batch 70 : 0.10372443497180939
Loss at batch 80 : 0.10392425954341888
Loss at batch 90 : 0.1320452094078064
Loss at batch 100 : 0.10568204522132874
Loss at batch 110 : 0.11472746729850769
Loss at batch 120 : 0.1284140795469284
Loss at batch 130 : 0.1131211519241333
Loss at batch 140 : 0.11405175179243088
Loss at batch 150 : 0.11359814554452896
Loss at batch 160 : 0.11110050231218338
Loss at batch 170 : 0.10002768784761429
Loss at batch 180 : 0.13066211342811584
Loss at batch 190 : 0.11024262756109238
Loss at batch 200 : 0.10438921302556992
Loss at batch 210 : 0.10002004355192184
Loss at batch 220 : 0.12299365550279617
Loss at batch 230 : 0.11322017014026642
Loss at batch 240 : 0.1012774333357811
Loss at batch 250 : 0.11063271760940552
Loss at batch 260 : 0.13401387631893158
Loss at batch 270 : 0.10940611362457275
Loss at batch 280 : 0.11112859100103378
Loss at batch 290 : 0.12255292385816574
Loss at batch 300 : 0.11726655066013336
Loss at batch 310 : 0.08242618292570114
Loss at batch 320 : 0.09837241470813751
Loss at batch 330 : 0.09910482168197632
Loss at batch 340 : 0.09729127585887909
Loss at batch 350 : 0.11611437052488327
Loss at batch 360 : 0.10438332706689835
Loss at batch 370 : 0.12152761965990067
epoch1 finished!
Loss at batch 10 : 0.11023658514022827
Loss at batch 20 : 0.10763387382030487
Loss at batch 30 : 0.09966858476400375
Loss at batch 40 : 0.1063336655497551
Loss at batch 50 : 0.11530996114015579
Loss at batch 60 : 0.11139500886201859
Loss at batch 70 : 0.11614543199539185
Loss at batch 80 : 0.10853220522403717
Loss at batch 90 : 0.10845284909009933
Loss at batch 100 : 0.12055957317352295
Loss at batch 110 : 0.07659893482923508
Loss at batch 120 : 0.03294072672724724
Loss at batch 130 : 0.03194229304790497
Loss at batch 140 : 0.019842948764562607
Loss at batch 150 : 0.02250957489013672
Loss at batch 160 : 0.05384013429284096
Loss at batch 170 : 0.016046253964304924
Loss at batch 180 : 0.02222054824233055
Loss at batch 190 : 0.020836595445871353
Loss at batch 200 : 0.024950724095106125
Loss at batch 210 : 0.020261285826563835
Loss at batch 220 : 0.02036634460091591
Loss at batch 230 : 0.014428459107875824
Loss at batch 240 : 0.019498826935887337
Loss at batch 250 : 0.020653588697314262
Loss at batch 260 : 0.02304184064269066
Loss at batch 270 : 0.018085353076457977
Loss at batch 280 : 0.024592749774456024
Loss at batch 290 : 0.020725449547171593
Loss at batch 300 : 0.019666576758027077
Loss at batch 310 : 0.026826100423932076
Loss at batch 320 : 0.025604121387004852
Loss at batch 330 : 0.026363851502537727
Loss at batch 340 : 0.028841238468885422
Loss at batch 350 : 0.03428136557340622
Loss at batch 360 : 0.017433207482099533
Loss at batch 370 : 0.022822758182883263
epoch2 finished!
Loss at batch 10 : 0.02753225527703762
Loss at batch 20 : 0.01826983131468296
Loss at batch 30 : 0.022905608639121056
Loss at batch 40 : 0.026120474562048912
Loss at batch 50 : 0.022192569449543953
Loss at batch 60 : 0.023107033222913742
Loss at batch 70 : 0.01998654566705227
Loss at batch 80 : 0.01446464192122221
Loss at batch 90 : 0.015629194676876068
Loss at batch 100 : 0.018540484830737114
Loss at batch 110 : 0.021367916837334633
Loss at batch 120 : 0.030179854482412338
Loss at batch 130 : 0.021999139338731766
Loss at batch 140 : 0.022779233753681183
Loss at batch 150 : 0.03738384321331978
Loss at batch 160 : 0.01977582275867462
Loss at batch 170 : 0.022672193124890327
Loss at batch 180 : 0.035984836518764496
Loss at batch 190 : 0.025605201721191406
Loss at batch 200 : 0.022499216720461845
Loss at batch 210 : 0.012255658395588398
Loss at batch 220 : 0.017424436286091805
Loss at batch 230 : 0.022761274129152298
Loss at batch 240 : 0.01738935150206089
Loss at batch 250 : 0.016169659793376923
Loss at batch 260 : 0.02116873301565647
Loss at batch 270 : 0.020258983597159386
Loss at batch 280 : 0.014929458498954773
Loss at batch 290 : 0.022416356950998306
Loss at batch 300 : 0.017365435138344765
Loss at batch 310 : 0.016986487433314323
Loss at batch 320 : 0.01695891283452511
Loss at batch 330 : 0.021274320781230927
Loss at batch 340 : 0.014747345820069313
Loss at batch 350 : 0.02879229746758938
Loss at batch 360 : 0.032592903822660446
Loss at batch 370 : 0.027737749740481377
epoch3 finished!
Loss at batch 10 : 0.01768318936228752
Loss at batch 20 : 0.02673509530723095
Loss at batch 30 : 0.020618531852960587
Loss at batch 40 : 0.015027706511318684
Loss at batch 50 : 0.03663821890950203
Loss at batch 60 : 0.027182836085557938
Loss at batch 70 : 0.030556052923202515
Loss at batch 80 : 0.01632501184940338
Loss at batch 90 : 0.015441750176250935
Loss at batch 100 : 0.01668819971382618
Loss at batch 110 : 0.017755623906850815
Loss at batch 120 : 0.03947874903678894
Loss at batch 130 : 0.017470207065343857
Loss at batch 140 : 0.018294140696525574
Loss at batch 150 : 0.019580252468585968
Loss at batch 160 : 0.014775367453694344
Loss at batch 170 : 0.018608873710036278
Loss at batch 180 : 0.0192499328404665
Loss at batch 190 : 0.024881472811102867
Loss at batch 200 : 0.017070062458515167
Loss at batch 210 : 0.022730082273483276
Loss at batch 220 : 0.01649491861462593
Loss at batch 230 : 0.016166916117072105
Loss at batch 240 : 0.022847015410661697
Loss at batch 250 : 0.02242746576666832
Loss at batch 260 : 0.021000074222683907
Loss at batch 270 : 0.02765895426273346
Loss at batch 280 : 0.02758396789431572
Loss at batch 290 : 0.015475636348128319
Loss at batch 300 : 0.012541267089545727
Loss at batch 310 : 0.025615591555833817
Loss at batch 320 : 0.024545997381210327
Loss at batch 330 : 0.02638142742216587
Loss at batch 340 : 0.023690592497587204
Loss at batch 350 : 0.018959179520606995
Loss at batch 360 : 0.022145263850688934
Loss at batch 370 : 0.02660309709608555
epoch4 finished!
Loss at batch 10 : 0.024823984131217003
Loss at batch 20 : 0.013023019768297672
Loss at batch 30 : 0.01728132739663124
Loss at batch 40 : 0.019786307588219643
Loss at batch 50 : 0.017061593011021614
Loss at batch 60 : 0.016655562445521355
Loss at batch 70 : 0.023151012137532234
Loss at batch 80 : 0.021683290600776672
Loss at batch 90 : 0.01680256426334381
Loss at batch 100 : 0.016194988042116165
Loss at batch 110 : 0.016063522547483444
Loss at batch 120 : 0.01907753385603428
Loss at batch 130 : 0.01731848157942295
Loss at batch 140 : 0.021382810547947884
Loss at batch 150 : 0.018497424200177193
Loss at batch 160 : 0.014825711026787758
Loss at batch 170 : 0.018713003024458885
Loss at batch 180 : 0.024342045187950134
Loss at batch 190 : 0.0238728616386652
Loss at batch 200 : 0.02230672538280487
Loss at batch 210 : 0.024430250748991966
Loss at batch 220 : 0.015904594212770462
Loss at batch 230 : 0.013692137785255909
Loss at batch 240 : 0.025692416355013847
Loss at batch 250 : 0.019674962386488914
Loss at batch 260 : 0.02452131174504757
Loss at batch 270 : 0.021695511415600777
Loss at batch 280 : 0.02919691614806652
Loss at batch 290 : 0.022910134866833687
Loss at batch 300 : 0.02187095768749714
Loss at batch 310 : 0.011181936599314213
Loss at batch 320 : 0.014207740314304829
Loss at batch 330 : 0.02240752801299095
Loss at batch 340 : 0.020246358588337898
Loss at batch 350 : 0.0404251404106617
Loss at batch 360 : 0.016102192923426628
Loss at batch 370 : 0.0190481785684824
epoch5 finished!
Loss at batch 10 : 0.033672429621219635
Loss at batch 20 : 0.01789572834968567
Loss at batch 30 : 0.016717642545700073
Loss at batch 40 : 0.01649353839457035
Loss at batch 50 : 0.02533620223402977
Loss at batch 60 : 0.029727580025792122
Loss at batch 70 : 0.02425759844481945
Loss at batch 80 : 0.013862205669283867
Loss at batch 90 : 0.013265052810311317
Loss at batch 100 : 0.021514400839805603
Loss at batch 110 : 0.018229346722364426
Loss at batch 120 : 0.020922014489769936
Loss at batch 130 : 0.029582634568214417
Loss at batch 140 : 0.019140206277370453
Loss at batch 150 : 0.02811030112206936
Loss at batch 160 : 0.016551893204450607
Loss at batch 170 : 0.018273087218403816
Loss at batch 180 : 0.021519647911190987
Loss at batch 190 : 0.01966826245188713
Loss at batch 200 : 0.01776915416121483
Loss at batch 210 : 0.02092093601822853
Loss at batch 220 : 0.017087779939174652
Loss at batch 230 : 0.026441849768161774
Loss at batch 240 : 0.014968926087021828
Loss at batch 250 : 0.027386153116822243
Loss at batch 260 : 0.025678414851427078
Loss at batch 270 : 0.014413461089134216
Loss at batch 280 : 0.020293910056352615
Loss at batch 290 : 0.01347621064633131
Loss at batch 300 : 0.012954034842550755
Loss at batch 310 : 0.02085244283080101
Loss at batch 320 : 0.022457851096987724
Loss at batch 330 : 0.015698371455073357
Loss at batch 340 : 0.018219411373138428
Loss at batch 350 : 0.019332915544509888
Loss at batch 360 : 0.017804600298404694
Loss at batch 370 : 0.021618181839585304
epoch6 finished!
Loss at batch 10 : 0.027119114995002747
Loss at batch 20 : 0.016416890546679497
Loss at batch 30 : 0.025494126603007317
Loss at batch 40 : 0.021917356178164482
Loss at batch 50 : 0.021544000133872032
Loss at batch 60 : 0.021146534010767937
Loss at batch 70 : 0.013056322932243347
Loss at batch 80 : 0.012968107126653194
Loss at batch 90 : 0.013964951038360596
Loss at batch 100 : 0.02063441462814808
Loss at batch 110 : 0.012482285499572754
Loss at batch 120 : 0.015227420255541801
Loss at batch 130 : 0.014636215753853321
Loss at batch 140 : 0.01672203466296196
Loss at batch 150 : 0.012193936854600906
Loss at batch 160 : 0.024348296225070953
Loss at batch 170 : 0.01203389372676611
Loss at batch 180 : 0.015672393143177032
Loss at batch 190 : 0.014773632399737835
Loss at batch 200 : 0.012284760363399982
Loss at batch 210 : 0.013840952888131142
Loss at batch 220 : 0.01322783250361681
Loss at batch 230 : 0.01517807599157095
Loss at batch 240 : 0.01976669393479824
Loss at batch 250 : 0.02497774362564087
Loss at batch 260 : 0.018535921350121498
Loss at batch 270 : 0.02023153193295002
Loss at batch 280 : 0.019890353083610535
Loss at batch 290 : 0.02139841765165329
Loss at batch 300 : 0.01855418272316456
Loss at batch 310 : 0.022373225539922714
Loss at batch 320 : 0.025614822283387184
Loss at batch 330 : 0.011072585359215736
Loss at batch 340 : 0.028814174234867096
Loss at batch 350 : 0.022361792623996735
Loss at batch 360 : 0.010328289121389389
Loss at batch 370 : 0.031840547919273376
epoch7 finished!
Loss at batch 10 : 0.02012096904218197
Loss at batch 20 : 0.01116274669766426
Loss at batch 30 : 0.014636204577982426
Loss at batch 40 : 0.022629503160715103
Loss at batch 50 : 0.016049519181251526
Loss at batch 60 : 0.012772965244948864
Loss at batch 70 : 0.02194785699248314
Loss at batch 80 : 0.024332571774721146
Loss at batch 90 : 0.012987914495170116
Loss at batch 100 : 0.0206711757928133
Loss at batch 110 : 0.01922435313463211
Loss at batch 120 : 0.027978943660855293
Loss at batch 130 : 0.0135954013094306
Loss at batch 140 : 0.02928991988301277
Loss at batch 150 : 0.025076663121581078
Loss at batch 160 : 0.015062283724546432
Loss at batch 170 : 0.026663411408662796
Loss at batch 180 : 0.015277479775249958
Loss at batch 190 : 0.017155004665255547
Loss at batch 200 : 0.009383167140185833
Loss at batch 210 : 0.01410833839327097
Loss at batch 220 : 0.015831248834729195
Loss at batch 230 : 0.02011631429195404
Loss at batch 240 : 0.0229911170899868
Loss at batch 250 : 0.015115411952137947
Loss at batch 260 : 0.01115428563207388
Loss at batch 270 : 0.022831862792372704
Loss at batch 280 : 0.03191792964935303
Loss at batch 290 : 0.019849663600325584
Loss at batch 300 : 0.013606789521872997
Loss at batch 310 : 0.020423633977770805
Loss at batch 320 : 0.016092805191874504
Loss at batch 330 : 0.02031126245856285
Loss at batch 340 : 0.021868007257580757
Loss at batch 350 : 0.011403425596654415
Loss at batch 360 : 0.018882043659687042
Loss at batch 370 : 0.012914702296257019
epoch8 finished!
Loss at batch 10 : 0.027499331161379814
Loss at batch 20 : 0.0217630323022604
Loss at batch 30 : 0.01568678952753544
Loss at batch 40 : 0.013918519020080566
Loss at batch 50 : 0.014184598810970783
Loss at batch 60 : 0.009374837391078472
Loss at batch 70 : 0.011673782020807266
Loss at batch 80 : 0.01927313022315502
Loss at batch 90 : 0.016721338033676147
Loss at batch 100 : 0.01287403330206871
Loss at batch 110 : 0.015670612454414368
Loss at batch 120 : 0.014851847663521767
Loss at batch 130 : 0.010840428061783314
Loss at batch 140 : 0.010907348245382309
Loss at batch 150 : 0.011556354351341724
Loss at batch 160 : 0.016271406784653664
Loss at batch 170 : 0.010290942154824734
Loss at batch 180 : 0.02468045987188816
Loss at batch 190 : 0.0202115960419178
Loss at batch 200 : 0.014349517412483692
Loss at batch 210 : 0.026782365515828133
Loss at batch 220 : 0.02643592841923237
Loss at batch 230 : 0.011479724198579788
Loss at batch 240 : 0.01271749846637249
Loss at batch 250 : 0.015484611503779888
Loss at batch 260 : 0.01333132665604353
Loss at batch 270 : 0.01080196164548397
Loss at batch 280 : 0.02766275592148304
Loss at batch 290 : 0.023028502240777016
Loss at batch 300 : 0.01665402390062809
Loss at batch 310 : 0.009530443698167801
Loss at batch 320 : 0.014276424422860146
Loss at batch 330 : 0.025485925376415253
Loss at batch 340 : 0.011138536967337132
Loss at batch 350 : 0.012250518426299095
Loss at batch 360 : 0.019868694245815277
Loss at batch 370 : 0.017583496868610382
epoch9 finished!
Loss at batch 10 : 0.018626460805535316
Loss at batch 20 : 0.017957886680960655
Loss at batch 30 : 0.020101500675082207
Loss at batch 40 : 0.02826673723757267
Loss at batch 50 : 0.015614386647939682
Loss at batch 60 : 0.00938892737030983
Loss at batch 70 : 0.013196398504078388
Loss at batch 80 : 0.02120591141283512
Loss at batch 90 : 0.016764888539910316
Loss at batch 100 : 0.02375660091638565
Loss at batch 110 : 0.020006541162729263
Loss at batch 120 : 0.018847716972231865
Loss at batch 130 : 0.012397391721606255
Loss at batch 140 : 0.015669221058487892
Loss at batch 150 : 0.013182275928556919
Loss at batch 160 : 0.013753797858953476
Loss at batch 170 : 0.018427696079015732
Loss at batch 180 : 0.016600124537944794
Loss at batch 190 : 0.010209638625383377
Loss at batch 200 : 0.013032997958362103
Loss at batch 210 : 0.013819094747304916
Loss at batch 220 : 0.011114111170172691
Loss at batch 230 : 0.021696200594305992
Loss at batch 240 : 0.011619135737419128
Loss at batch 250 : 0.017408354207873344
Loss at batch 260 : 0.029600709676742554
Loss at batch 270 : 0.02627747692167759
Loss at batch 280 : 0.015218673273921013
Loss at batch 290 : 0.019829152151942253
Loss at batch 300 : 0.015716083347797394
Loss at batch 310 : 0.020827019587159157
Loss at batch 320 : 0.01435293909162283
Loss at batch 330 : 0.022536152973771095
Loss at batch 340 : 0.023320496082305908
Loss at batch 350 : 0.020169854164123535
Loss at batch 360 : 0.016348110511898994
Loss at batch 370 : 0.018855350092053413
epoch10 finished!
Loss at batch 10 : 0.012618295848369598
Loss at batch 20 : 0.021230820566415787
Loss at batch 30 : 0.01420403178781271
Loss at batch 40 : 0.01061088778078556
Loss at batch 50 : 0.01965390518307686
Loss at batch 60 : 0.024812618270516396
Loss at batch 70 : 0.010920954868197441
Loss at batch 80 : 0.020285770297050476
Loss at batch 90 : 0.020311648026108742
Loss at batch 100 : 0.01176978275179863
Loss at batch 110 : 0.027097728103399277
Loss at batch 120 : 0.024725724011659622
Loss at batch 130 : 0.021226536482572556
Loss at batch 140 : 0.009196196682751179
Loss at batch 150 : 0.023202551528811455
Loss at batch 160 : 0.014564336277544498
Loss at batch 170 : 0.017615821212530136
Loss at batch 180 : 0.02417628839612007
Loss at batch 190 : 0.027465401217341423
Loss at batch 200 : 0.018663370981812477
Loss at batch 210 : 0.02264491654932499
Loss at batch 220 : 0.0152957858517766
Loss at batch 230 : 0.010628771968185902
Loss at batch 240 : 0.01622694730758667
Loss at batch 250 : 0.02209417149424553
Loss at batch 260 : 0.0328778512775898
Loss at batch 270 : 0.022348035126924515
Loss at batch 280 : 0.01997821405529976
Loss at batch 290 : 0.017297063022851944
Loss at batch 300 : 0.014230089262127876
Loss at batch 310 : 0.016471395269036293
Loss at batch 320 : 0.010205559432506561
Loss at batch 330 : 0.011741070076823235
Loss at batch 340 : 0.018148936331272125
Loss at batch 350 : 0.01398673839867115
Loss at batch 360 : 0.02135269157588482
Loss at batch 370 : 0.021607212722301483
epoch11 finished!
Loss at batch 10 : 0.009007815271615982
Loss at batch 20 : 0.02734694443643093
Loss at batch 30 : 0.015341182239353657
Loss at batch 40 : 0.02202059142291546
Loss at batch 50 : 0.017499662935733795
Loss at batch 60 : 0.0320759192109108
Loss at batch 70 : 0.016749361529946327
Loss at batch 80 : 0.011768406257033348
Loss at batch 90 : 0.028440482914447784
Loss at batch 100 : 0.022557225078344345
Loss at batch 110 : 0.012826226651668549
Loss at batch 120 : 0.017680393531918526
Loss at batch 130 : 0.012622112408280373
Loss at batch 140 : 0.014657762832939625
Loss at batch 150 : 0.02444152906537056
Loss at batch 160 : 0.0148072624579072
Loss at batch 170 : 0.015999089926481247
Loss at batch 180 : 0.014557352289557457
Loss at batch 190 : 0.01554825622588396
Loss at batch 200 : 0.014509089291095734
Loss at batch 210 : 0.011906150728464127
Loss at batch 220 : 0.018142670392990112
Loss at batch 230 : 0.018381407484412193
Loss at batch 240 : 0.025942033156752586
Loss at batch 250 : 0.010983525775372982
Loss at batch 260 : 0.022870615124702454
Loss at batch 270 : 0.018319040536880493
Loss at batch 280 : 0.015065746381878853
Loss at batch 290 : 0.014762677252292633
Loss at batch 300 : 0.036615803837776184
Loss at batch 310 : 0.020212598145008087
Loss at batch 320 : 0.014361580833792686
Loss at batch 330 : 0.03661976009607315
Loss at batch 340 : 0.017492050305008888
Loss at batch 350 : 0.013348844833672047
Loss at batch 360 : 0.013541951775550842
Loss at batch 370 : 0.01626003161072731
epoch12 finished!
Loss at batch 10 : 0.019732976332306862
Loss at batch 20 : 0.016993379220366478
Loss at batch 30 : 0.015193511731922626
Loss at batch 40 : 0.018668167293071747
Loss at batch 50 : 0.01947788894176483
Loss at batch 60 : 0.021341243758797646
Loss at batch 70 : 0.02607239969074726
Loss at batch 80 : 0.017667695879936218
Loss at batch 90 : 0.02151857689023018
Loss at batch 100 : 0.029135216027498245
Loss at batch 110 : 0.02538798563182354
Loss at batch 120 : 0.010780355893075466
Loss at batch 130 : 0.012405775487422943
Loss at batch 140 : 0.01934749074280262
Loss at batch 150 : 0.009043579921126366
Loss at batch 160 : 0.017193960025906563
Loss at batch 170 : 0.029749253764748573
Loss at batch 180 : 0.014632327482104301
Loss at batch 190 : 0.018843451514840126
Loss at batch 200 : 0.02560773305594921
Loss at batch 210 : 0.01320508774369955
Loss at batch 220 : 0.010351154021918774
Loss at batch 230 : 0.016107888892292976
Loss at batch 240 : 0.018981769680976868
Loss at batch 250 : 0.009423189796507359
Loss at batch 260 : 0.01295360829681158
Loss at batch 270 : 0.017099613323807716
Loss at batch 280 : 0.01031623687595129
Loss at batch 290 : 0.019922327250242233
Loss at batch 300 : 0.013439436443150043
Loss at batch 310 : 0.03789166733622551
Loss at batch 320 : 0.014149603433907032
Loss at batch 330 : 0.009805465117096901
Loss at batch 340 : 0.014599700458347797
Loss at batch 350 : 0.016446562483906746
Loss at batch 360 : 0.016475116834044456
Loss at batch 370 : 0.019096625968813896
epoch13 finished!
Loss at batch 10 : 0.010644359514117241
Loss at batch 20 : 0.02081388235092163
Loss at batch 30 : 0.012283139862120152
Loss at batch 40 : 0.024381892755627632
Loss at batch 50 : 0.022481409832835197
Loss at batch 60 : 0.013428577221930027
Loss at batch 70 : 0.03857632726430893
Loss at batch 80 : 0.010191930457949638
Loss at batch 90 : 0.014398755505681038
Loss at batch 100 : 0.014882383868098259
Loss at batch 110 : 0.014716585166752338
Loss at batch 120 : 0.012744704261422157
Loss at batch 130 : 0.015237757936120033
Loss at batch 140 : 0.012099211104214191
Loss at batch 150 : 0.025423889979720116
Loss at batch 160 : 0.029366176575422287
Loss at batch 170 : 0.01779777556657791
Loss at batch 180 : 0.02163826860487461
Loss at batch 190 : 0.013713796623051167
Loss at batch 200 : 0.010476903058588505
Loss at batch 210 : 0.01378251239657402
Loss at batch 220 : 0.027507903054356575
Loss at batch 230 : 0.015086100436747074
Loss at batch 240 : 0.014459756202995777
Loss at batch 250 : 0.017267515882849693
Loss at batch 260 : 0.01622961089015007
Loss at batch 270 : 0.013965501450002193
Loss at batch 280 : 0.02070624567568302
Loss at batch 290 : 0.022554393857717514
Loss at batch 300 : 0.02857704646885395
Loss at batch 310 : 0.015669219195842743
Loss at batch 320 : 0.022764749825000763
Loss at batch 330 : 0.017567971721291542
Loss at batch 340 : 0.01963106170296669
Loss at batch 350 : 0.01898004300892353
Loss at batch 360 : 0.027528084814548492
Loss at batch 370 : 0.014676488935947418
epoch14 finished!
Loss at batch 10 : 0.02279890514910221
Loss at batch 20 : 0.02866818942129612
Loss at batch 30 : 0.015762047842144966
Loss at batch 40 : 0.014085259288549423
Loss at batch 50 : 0.01186855137348175
Loss at batch 60 : 0.011266243644058704
Loss at batch 70 : 0.01411440595984459
Loss at batch 80 : 0.01069628819823265
Loss at batch 90 : 0.01517738588154316
Loss at batch 100 : 0.009878650307655334
Loss at batch 110 : 0.014131457544863224
Loss at batch 120 : 0.012222487479448318
Loss at batch 130 : 0.015430326573550701
Loss at batch 140 : 0.042132340371608734
Loss at batch 150 : 0.015480938367545605
Loss at batch 160 : 0.015817634761333466
Loss at batch 170 : 0.017000475898385048
Loss at batch 180 : 0.015275616198778152
Loss at batch 190 : 0.011120183393359184
Loss at batch 200 : 0.018746543675661087
Loss at batch 210 : 0.02293488010764122
Loss at batch 220 : 0.021382177248597145
Loss at batch 230 : 0.010511242784559727
Loss at batch 240 : 0.022457068786025047
Loss at batch 250 : 0.013677217066287994
Loss at batch 260 : 0.018032299354672432
Loss at batch 270 : 0.013691136613488197
Loss at batch 280 : 0.013260734267532825
Loss at batch 290 : 0.012606697157025337
Loss at batch 300 : 0.01925639994442463
Loss at batch 310 : 0.011433548294007778
Loss at batch 320 : 0.01011111494153738
Loss at batch 330 : 0.019317524507641792
Loss at batch 340 : 0.0244765505194664
Loss at batch 350 : 0.020141564309597015
Loss at batch 360 : 0.012228133156895638
Loss at batch 370 : 0.020916525274515152
epoch15 finished!
Loss at batch 10 : 0.011983931064605713
Loss at batch 20 : 0.008780690841376781
Loss at batch 30 : 0.019966011866927147
Loss at batch 40 : 0.01355959102511406
Loss at batch 50 : 0.026091882959008217
Loss at batch 60 : 0.021633068099617958
Loss at batch 70 : 0.008347006514668465
Loss at batch 80 : 0.015824364498257637
Loss at batch 90 : 0.02247105911374092
Loss at batch 100 : 0.027919527143239975
Loss at batch 110 : 0.018917270004749298
Loss at batch 120 : 0.016114644706249237
Loss at batch 130 : 0.009053713642060757
Loss at batch 140 : 0.011822995729744434
Loss at batch 150 : 0.02585316263139248
Loss at batch 160 : 0.01904279738664627
Loss at batch 170 : 0.010663967579603195
Loss at batch 180 : 0.012895888648927212
Loss at batch 190 : 0.016726437956094742
Loss at batch 200 : 0.01387149840593338
Loss at batch 210 : 0.02951808087527752
Loss at batch 220 : 0.01788259483873844
Loss at batch 230 : 0.021215559914708138
Loss at batch 240 : 0.009782564826309681
Loss at batch 250 : 0.012248698621988297
Loss at batch 260 : 0.011221126653254032
Loss at batch 270 : 0.016949038952589035
Loss at batch 280 : 0.019568419083952904
Loss at batch 290 : 0.011775244027376175
Loss at batch 300 : 0.016017628833651543
Loss at batch 310 : 0.02813650108873844
Loss at batch 320 : 0.01698722317814827
Loss at batch 330 : 0.02361750416457653
Loss at batch 340 : 0.012481656856834888
Loss at batch 350 : 0.024161361157894135
Loss at batch 360 : 0.013928087428212166
Loss at batch 370 : 0.010687783360481262
epoch16 finished!
Loss at batch 10 : 0.014244013465940952
Loss at batch 20 : 0.017864814028143883
Loss at batch 30 : 0.01364994514733553
Loss at batch 40 : 0.02052292600274086
Loss at batch 50 : 0.012331665493547916
Loss at batch 60 : 0.017297083511948586
Loss at batch 70 : 0.01413056068122387
Loss at batch 80 : 0.011589465662837029
Loss at batch 90 : 0.012821085751056671
Loss at batch 100 : 0.015125641599297523
Loss at batch 110 : 0.009968463331460953
Loss at batch 120 : 0.01857757195830345
Loss at batch 130 : 0.02126281149685383
Loss at batch 140 : 0.022973837330937386
Loss at batch 150 : 0.009392932057380676
Loss at batch 160 : 0.014916051179170609
Loss at batch 170 : 0.01704820618033409
Loss at batch 180 : 0.010446417145431042
Loss at batch 190 : 0.012310346588492393
Loss at batch 200 : 0.019595477730035782
Loss at batch 210 : 0.016359714791178703
Loss at batch 220 : 0.016513163223862648
Loss at batch 230 : 0.011311499401926994
Loss at batch 240 : 0.011345096863806248
Loss at batch 250 : 0.014969460666179657
Loss at batch 260 : 0.017507372424006462
Loss at batch 270 : 0.017511574551463127
Loss at batch 280 : 0.014963453635573387
Loss at batch 290 : 0.018953291699290276
Loss at batch 300 : 0.01861707493662834
Loss at batch 310 : 0.01706055924296379
Loss at batch 320 : 0.025437232106924057
Loss at batch 330 : 0.01592351123690605
Loss at batch 340 : 0.026365308091044426
Loss at batch 350 : 0.022415827959775925
Loss at batch 360 : 0.02351231873035431
Loss at batch 370 : 0.014739248901605606
epoch17 finished!
Loss at batch 10 : 0.020001450553536415
Loss at batch 20 : 0.012433943338692188
Loss at batch 30 : 0.017189759761095047
Loss at batch 40 : 0.019394362345337868
Loss at batch 50 : 0.019353719428181648
Loss at batch 60 : 0.028589265421032906
Loss at batch 70 : 0.01325819082558155
Loss at batch 80 : 0.01332761812955141
Loss at batch 90 : 0.015571363270282745
Loss at batch 100 : 0.00972865242511034
Loss at batch 110 : 0.015278146602213383
Loss at batch 120 : 0.011930488981306553
Loss at batch 130 : 0.015453227795660496
Loss at batch 140 : 0.015241669490933418
Loss at batch 150 : 0.010738381184637547
Loss at batch 160 : 0.010726604610681534
Loss at batch 170 : 0.017796790227293968
Loss at batch 180 : 0.011630930006504059
Loss at batch 190 : 0.022989919409155846
Loss at batch 200 : 0.013225237838923931
Loss at batch 210 : 0.013855942524969578
Loss at batch 220 : 0.019986111670732498
Loss at batch 230 : 0.00878573302179575
Loss at batch 240 : 0.016447121277451515
Loss at batch 250 : 0.010786428116261959
Loss at batch 260 : 0.011787937954068184
Loss at batch 270 : 0.014791719615459442
Loss at batch 280 : 0.012608231976628304
Loss at batch 290 : 0.017161089926958084
Loss at batch 300 : 0.016705946996808052
Loss at batch 310 : 0.013037057593464851
Loss at batch 320 : 0.017854828387498856
Loss at batch 330 : 0.01647856831550598
Loss at batch 340 : 0.01141791045665741
Loss at batch 350 : 0.012975642457604408
Loss at batch 360 : 0.019945954903960228
Loss at batch 370 : 0.01141318865120411
epoch18 finished!
Loss at batch 10 : 0.016951417550444603
Loss at batch 20 : 0.0230381079018116
Loss at batch 30 : 0.009662267751991749
Loss at batch 40 : 0.01755157671868801
Loss at batch 50 : 0.015122387558221817
Loss at batch 60 : 0.015562168322503567
Loss at batch 70 : 0.01810494437813759
Loss at batch 80 : 0.01701170764863491
Loss at batch 90 : 0.013263733126223087
Loss at batch 100 : 0.010734403505921364
Loss at batch 110 : 0.01703769899904728
Loss at batch 120 : 0.01750769279897213
Loss at batch 130 : 0.016396421939134598
Loss at batch 140 : 0.011661766096949577
Loss at batch 150 : 0.017509398981928825
Loss at batch 160 : 0.012920076958835125
Loss at batch 170 : 0.02468924969434738
Loss at batch 180 : 0.020127857103943825
Loss at batch 190 : 0.00795617327094078
Loss at batch 200 : 0.017028389498591423
Loss at batch 210 : 0.017315318807959557
Loss at batch 220 : 0.014449933543801308
Loss at batch 230 : 0.01669166050851345
Loss at batch 240 : 0.033597640693187714
Loss at batch 250 : 0.014964957721531391
Loss at batch 260 : 0.016896992921829224
Loss at batch 270 : 0.010272104293107986
Loss at batch 280 : 0.020333968102931976
Loss at batch 290 : 0.023313751444220543
Loss at batch 300 : 0.01597958616912365
Loss at batch 310 : 0.02161993272602558
Loss at batch 320 : 0.019066935405135155
Loss at batch 330 : 0.017667409032583237
Loss at batch 340 : 0.013258040882647038
Loss at batch 350 : 0.011625525541603565
Loss at batch 360 : 0.022846972569823265
Loss at batch 370 : 0.020580720156431198
epoch19 finished!
Loss at batch 10 : 0.024092717096209526
Loss at batch 20 : 0.01535735372453928
Loss at batch 30 : 0.013060538098216057
Loss at batch 40 : 0.009083900600671768
Loss at batch 50 : 0.016300855204463005
Loss at batch 60 : 0.018703460693359375
Loss at batch 70 : 0.019881561398506165
Loss at batch 80 : 0.009540822356939316
Loss at batch 90 : 0.00804980006068945
Loss at batch 100 : 0.015117159113287926
Loss at batch 110 : 0.019038716331124306
Loss at batch 120 : 0.014356259256601334
Loss at batch 130 : 0.02469640225172043
Loss at batch 140 : 0.016562359407544136
Loss at batch 150 : 0.024295946583151817
Loss at batch 160 : 0.025106780230998993
Loss at batch 170 : 0.008859576657414436
Loss at batch 180 : 0.011024598963558674
Loss at batch 190 : 0.02411717362701893
Loss at batch 200 : 0.01849292777478695
Loss at batch 210 : 0.015070129185914993
Loss at batch 220 : 0.010746383108198643
Loss at batch 230 : 0.01277199573814869
Loss at batch 240 : 0.01436567585915327
Loss at batch 250 : 0.01791466772556305
Loss at batch 260 : 0.013877674005925655
Loss at batch 270 : 0.023489823564887047
Loss at batch 280 : 0.018536783754825592
Loss at batch 290 : 0.015195030719041824
Loss at batch 300 : 0.019009001553058624
Loss at batch 310 : 0.014557898044586182
Loss at batch 320 : 0.018876701593399048
Loss at batch 330 : 0.007658509537577629
Loss at batch 340 : 0.011733521707355976
Loss at batch 350 : 0.016006970778107643
Loss at batch 360 : 0.010110002011060715
Loss at batch 370 : 0.01242376770824194
epoch20 finished!
Loss at batch 10 : 0.012631204910576344
Loss at batch 20 : 0.014989129267632961
Loss at batch 30 : 0.008167791180312634
Loss at batch 40 : 0.010219212621450424
Loss at batch 50 : 0.019738350063562393
Loss at batch 60 : 0.021740959957242012
Loss at batch 70 : 0.01889844425022602
Loss at batch 80 : 0.0071998657658696175
Loss at batch 90 : 0.01519634947180748
Loss at batch 100 : 0.03018380142748356
Loss at batch 110 : 0.015736814588308334
Loss at batch 120 : 0.02393331564962864
Loss at batch 130 : 0.01921330951154232
Loss at batch 140 : 0.008748178370296955
Loss at batch 150 : 0.029034985229372978
Loss at batch 160 : 0.016584191471338272
Loss at batch 170 : 0.014532694593071938
Loss at batch 180 : 0.01780707575380802
Loss at batch 190 : 0.018934311345219612
Loss at batch 200 : 0.017125315964221954
Loss at batch 210 : 0.015487411059439182
Loss at batch 220 : 0.014932828024029732
Loss at batch 230 : 0.01865389570593834
Loss at batch 240 : 0.016888948157429695
Loss at batch 250 : 0.019736580550670624
Loss at batch 260 : 0.023545926436781883
Loss at batch 270 : 0.015160202980041504
Loss at batch 280 : 0.029159940779209137
Loss at batch 290 : 0.015315194614231586
Loss at batch 300 : 0.025971634313464165
Loss at batch 310 : 0.008895977400243282
Loss at batch 320 : 0.015319970436394215
Loss at batch 330 : 0.016153018921613693
Loss at batch 340 : 0.01964196190237999
Loss at batch 350 : 0.027910253033041954
Loss at batch 360 : 0.011255290359258652
Loss at batch 370 : 0.022630149498581886
epoch21 finished!
Loss at batch 10 : 0.021724820137023926
Loss at batch 20 : 0.008537949994206429
Loss at batch 30 : 0.009338201023638248
Loss at batch 40 : 0.01724262721836567
Loss at batch 50 : 0.020843708887696266
Loss at batch 60 : 0.012292610481381416
Loss at batch 70 : 0.02385590225458145
Loss at batch 80 : 0.0174164567142725
Loss at batch 90 : 0.01956913061439991
Loss at batch 100 : 0.021421102806925774
Loss at batch 110 : 0.021995294839143753
Loss at batch 120 : 0.009528079070150852
Loss at batch 130 : 0.020405586808919907
Loss at batch 140 : 0.01984855905175209
Loss at batch 150 : 0.01300010271370411
Loss at batch 160 : 0.01955553889274597
Loss at batch 170 : 0.013897694647312164
Loss at batch 180 : 0.010813642293214798
Loss at batch 190 : 0.0075027355924248695
Loss at batch 200 : 0.01568438485264778
Loss at batch 210 : 0.01737334579229355
Loss at batch 220 : 0.014284465461969376
Loss at batch 230 : 0.016924012452363968
Loss at batch 240 : 0.0167393796145916
Loss at batch 250 : 0.011037560179829597
Loss at batch 260 : 0.01886630617082119
Loss at batch 270 : 0.015661073848605156
Loss at batch 280 : 0.015252649784088135
Loss at batch 290 : 0.013357673771679401
Loss at batch 300 : 0.026567747816443443
Loss at batch 310 : 0.015370117500424385
Loss at batch 320 : 0.009746905416250229
Loss at batch 330 : 0.013605556450784206
Loss at batch 340 : 0.02590465545654297
Loss at batch 350 : 0.012526808306574821
Loss at batch 360 : 0.015828337520360947
Loss at batch 370 : 0.010715343989431858
epoch22 finished!
Loss at batch 10 : 0.01761281117796898
Loss at batch 20 : 0.0176443699747324
Loss at batch 30 : 0.014291253872215748
Loss at batch 40 : 0.025128407403826714
Loss at batch 50 : 0.035697948187589645
Loss at batch 60 : 0.021579332649707794
Loss at batch 70 : 0.015900200232863426
Loss at batch 80 : 0.016345825046300888
Loss at batch 90 : 0.008832147344946861
Loss at batch 100 : 0.015369979664683342
Loss at batch 110 : 0.016201671212911606
Loss at batch 120 : 0.01227197889238596
Loss at batch 130 : 0.015151026658713818
Loss at batch 140 : 0.014884091913700104
Loss at batch 150 : 0.017497217282652855
Loss at batch 160 : 0.015183071605861187
Loss at batch 170 : 0.02122839167714119
Loss at batch 180 : 0.026234962046146393
Loss at batch 190 : 0.008896132931113243
Loss at batch 200 : 0.01877719908952713
Loss at batch 210 : 0.007675792556256056
Loss at batch 220 : 0.014489171095192432
Loss at batch 230 : 0.018321186304092407
Loss at batch 240 : 0.008975432254374027
Loss at batch 250 : 0.01356353610754013
Loss at batch 260 : 0.012700475752353668
Loss at batch 270 : 0.01605374552309513
Loss at batch 280 : 0.014334422536194324
Loss at batch 290 : 0.012179902754724026
Loss at batch 300 : 0.012659369967877865
Loss at batch 310 : 0.02297988161444664
Loss at batch 320 : 0.013040860183537006
Loss at batch 330 : 0.013748069293797016
Loss at batch 340 : 0.01763882301747799
Loss at batch 350 : 0.019435925409197807
Loss at batch 360 : 0.01891174353659153
Loss at batch 370 : 0.01713033951818943
epoch23 finished!
Loss at batch 10 : 0.019059237092733383
Loss at batch 20 : 0.012007120065391064
Loss at batch 30 : 0.01858370564877987
Loss at batch 40 : 0.010974260978400707
Loss at batch 50 : 0.023805994540452957
Loss at batch 60 : 0.009024539962410927
Loss at batch 70 : 0.015492357313632965
Loss at batch 80 : 0.017754007130861282
Loss at batch 90 : 0.016664503142237663
Loss at batch 100 : 0.019046755507588387
Loss at batch 110 : 0.008859969675540924
Loss at batch 120 : 0.011072572320699692
Loss at batch 130 : 0.02580985240638256
Loss at batch 140 : 0.026666484773159027
Loss at batch 150 : 0.013538488186895847
Loss at batch 160 : 0.010789783671498299
Loss at batch 170 : 0.020118696615099907
Loss at batch 180 : 0.012050915509462357
Loss at batch 190 : 0.018266398459672928
Loss at batch 200 : 0.010932805947959423
Loss at batch 210 : 0.014392534270882607
Loss at batch 220 : 0.016219263896346092
Loss at batch 230 : 0.010775015689432621
Loss at batch 240 : 0.022931788116693497
Loss at batch 250 : 0.016228029504418373
Loss at batch 260 : 0.010075408965349197
Loss at batch 270 : 0.012806153856217861
Loss at batch 280 : 0.009273984469473362
Loss at batch 290 : 0.018551301211118698
Loss at batch 300 : 0.013939591124653816
Loss at batch 310 : 0.016924820840358734
Loss at batch 320 : 0.023794954642653465
Loss at batch 330 : 0.016848159953951836
Loss at batch 340 : 0.014757410623133183
Loss at batch 350 : 0.008991716429591179
Loss at batch 360 : 0.02360941469669342
Loss at batch 370 : 0.014941902831196785
epoch24 finished!
Loss at batch 10 : 0.011756137944757938
Loss at batch 20 : 0.01788271591067314
Loss at batch 30 : 0.009549278765916824
Loss at batch 40 : 0.017741940915584564
Loss at batch 50 : 0.025448838248848915
Loss at batch 60 : 0.013452833518385887
Loss at batch 70 : 0.009293467737734318
Loss at batch 80 : 0.019163833931088448
Loss at batch 90 : 0.018046434968709946
Loss at batch 100 : 0.01917940191924572
Loss at batch 110 : 0.009412011131644249
Loss at batch 120 : 0.030613631010055542
Loss at batch 130 : 0.013848178088665009
Loss at batch 140 : 0.017180390655994415
Loss at batch 150 : 0.015549743548035622
Loss at batch 160 : 0.0148166473954916
Loss at batch 170 : 0.009478675201535225
Loss at batch 180 : 0.012179919518530369
Loss at batch 190 : 0.025801317766308784
Loss at batch 200 : 0.02325938455760479
Loss at batch 210 : 0.019065991044044495
Loss at batch 220 : 0.020022500306367874
Loss at batch 230 : 0.013302860781550407
Loss at batch 240 : 0.009534673765301704
Loss at batch 250 : 0.017806099727749825
Loss at batch 260 : 0.012387733906507492
Loss at batch 270 : 0.01566692441701889
Loss at batch 280 : 0.008216144517064095
Loss at batch 290 : 0.016851473599672318
Loss at batch 300 : 0.011534713208675385
Loss at batch 310 : 0.013111734762787819
Loss at batch 320 : 0.010359067469835281
Loss at batch 330 : 0.012245877645909786
Loss at batch 340 : 0.011082526296377182
Loss at batch 350 : 0.016087517142295837
Loss at batch 360 : 0.019671078771352768
Loss at batch 370 : 0.020207557827234268
epoch25 finished!
Loss at batch 10 : 0.01525304839015007
Loss at batch 20 : 0.015935759991407394
Loss at batch 30 : 0.01999284140765667
Loss at batch 40 : 0.012011957354843616
Loss at batch 50 : 0.011444629170000553
Loss at batch 60 : 0.021643036976456642
Loss at batch 70 : 0.01043347455561161
Loss at batch 80 : 0.01969805732369423
Loss at batch 90 : 0.019976552575826645
Loss at batch 100 : 0.011979956179857254
Loss at batch 110 : 0.014503947459161282
Loss at batch 120 : 0.009033845737576485
Loss at batch 130 : 0.007029700092971325
Loss at batch 140 : 0.00970188807696104
Loss at batch 150 : 0.01671343669295311
Loss at batch 160 : 0.021290112286806107
Loss at batch 170 : 0.022985825315117836
Loss at batch 180 : 0.011057619005441666
Loss at batch 190 : 0.017662424594163895
Loss at batch 200 : 0.013724559918045998
Loss at batch 210 : 0.011180511675775051
Loss at batch 220 : 0.01609921269118786
Loss at batch 230 : 0.011504940688610077
Loss at batch 240 : 0.025431986898183823
Loss at batch 250 : 0.020276624709367752
Loss at batch 260 : 0.02654205821454525
Loss at batch 270 : 0.023342663422226906
Loss at batch 280 : 0.010242820717394352
Loss at batch 290 : 0.013320136815309525
Loss at batch 300 : 0.01879492774605751
Loss at batch 310 : 0.018622975796461105
Loss at batch 320 : 0.01622902974486351
Loss at batch 330 : 0.012389387004077435
Loss at batch 340 : 0.015296618454158306
Loss at batch 350 : 0.008157539181411266
Loss at batch 360 : 0.00817287527024746
Loss at batch 370 : 0.011554845608770847
epoch26 finished!
Loss at batch 10 : 0.01829865761101246
Loss at batch 20 : 0.011217623949050903
Loss at batch 30 : 0.008619243279099464
Loss at batch 40 : 0.008788700215518475
Loss at batch 50 : 0.01702248491346836
Loss at batch 60 : 0.007166159804910421
Loss at batch 70 : 0.03126582130789757
Loss at batch 80 : 0.010348238050937653
Loss at batch 90 : 0.015450255014002323
Loss at batch 100 : 0.01589059829711914
Loss at batch 110 : 0.01004034373909235
Loss at batch 120 : 0.014179087243974209
Loss at batch 130 : 0.014192651025950909
Loss at batch 140 : 0.019039563834667206
Loss at batch 150 : 0.019929949194192886
Loss at batch 160 : 0.018240639939904213
Loss at batch 170 : 0.01016908697783947
Loss at batch 180 : 0.01331407018005848
Loss at batch 190 : 0.014764277264475822
Loss at batch 200 : 0.014855023473501205
Loss at batch 210 : 0.013830842450261116
Loss at batch 220 : 0.02887386456131935
Loss at batch 230 : 0.013150873593986034
Loss at batch 240 : 0.024582387879490852
Loss at batch 250 : 0.0072190891951322556
Loss at batch 260 : 0.01283256895840168
Loss at batch 270 : 0.02284007892012596
Loss at batch 280 : 0.010262048803269863
Loss at batch 290 : 0.011751633137464523
Loss at batch 300 : 0.0225773137062788
Loss at batch 310 : 0.009909669868648052
Loss at batch 320 : 0.017025042325258255
Loss at batch 330 : 0.013007492758333683
Loss at batch 340 : 0.01589425839483738
Loss at batch 350 : 0.015959784388542175
Loss at batch 360 : 0.01703949272632599
Loss at batch 370 : 0.011954434216022491
epoch27 finished!
Loss at batch 10 : 0.02220425009727478
Loss at batch 20 : 0.024326393380761147
Loss at batch 30 : 0.01631748117506504
Loss at batch 40 : 0.013445282354950905
Loss at batch 50 : 0.013660854659974575
Loss at batch 60 : 0.019137268885970116
Loss at batch 70 : 0.015312583185732365
Loss at batch 80 : 0.011139227077364922
Loss at batch 90 : 0.017674148082733154
Loss at batch 100 : 0.01156037487089634
Loss at batch 110 : 0.010417677462100983
Loss at batch 120 : 0.011157354339957237
Loss at batch 130 : 0.01690753363072872
Loss at batch 140 : 0.009267790243029594
Loss at batch 150 : 0.028040368109941483
Loss at batch 160 : 0.013577237725257874
Loss at batch 170 : 0.011334514245390892
Loss at batch 180 : 0.02332914248108864
Loss at batch 190 : 0.00852916669100523
Loss at batch 200 : 0.024966614320874214
Loss at batch 210 : 0.012564748525619507
Loss at batch 220 : 0.005964093841612339
Loss at batch 230 : 0.012214366346597672
Loss at batch 240 : 0.011113820597529411
Loss at batch 250 : 0.02015264704823494
Loss at batch 260 : 0.018729034811258316
Loss at batch 270 : 0.0056922659277915955
Loss at batch 280 : 0.014859586022794247
Loss at batch 290 : 0.02347557246685028
Loss at batch 300 : 0.02344650775194168
Loss at batch 310 : 0.0122635867446661
Loss at batch 320 : 0.014200539328157902
Loss at batch 330 : 0.019980207085609436
Loss at batch 340 : 0.023134266957640648
Loss at batch 350 : 0.011930033564567566
Loss at batch 360 : 0.011295553296804428
Loss at batch 370 : 0.016075411811470985
epoch28 finished!
Loss at batch 10 : 0.017337927594780922
Loss at batch 20 : 0.015255825594067574
Loss at batch 30 : 0.03115542232990265
Loss at batch 40 : 0.01718742400407791
Loss at batch 50 : 0.012540729716420174
Loss at batch 60 : 0.015321081504225731
Loss at batch 70 : 0.015548981726169586
Loss at batch 80 : 0.010364740155637264
Loss at batch 90 : 0.01119405310600996
Loss at batch 100 : 0.028471507132053375
Loss at batch 110 : 0.014570442028343678
Loss at batch 120 : 0.026792870834469795
Loss at batch 130 : 0.012974893674254417
Loss at batch 140 : 0.013438384048640728
Loss at batch 150 : 0.008284538052976131
Loss at batch 160 : 0.012847923673689365
Loss at batch 170 : 0.012703497894108295
Loss at batch 180 : 0.012790185399353504
Loss at batch 190 : 0.01085888035595417
Loss at batch 200 : 0.024528488516807556
Loss at batch 210 : 0.014955595135688782
Loss at batch 220 : 0.01126139983534813
Loss at batch 230 : 0.014234672300517559
Loss at batch 240 : 0.010997740551829338
Loss at batch 250 : 0.012523097917437553
Loss at batch 260 : 0.01672293432056904
Loss at batch 270 : 0.01394982822239399
Loss at batch 280 : 0.017642419785261154
Loss at batch 290 : 0.016367705538868904
Loss at batch 300 : 0.01619587279856205
Loss at batch 310 : 0.014016124419867992
Loss at batch 320 : 0.010694538243114948
Loss at batch 330 : 0.01163432002067566
Loss at batch 340 : 0.017461145296692848
Loss at batch 350 : 0.016135727986693382
Loss at batch 360 : 0.01219622977077961
Loss at batch 370 : 0.009871037676930428
epoch29 finished!
Loss at batch 10 : 0.016603708267211914
Loss at batch 20 : 0.018514394760131836
Loss at batch 30 : 0.017562124878168106
Loss at batch 40 : 0.012324818409979343
Loss at batch 50 : 0.01605726033449173
Loss at batch 60 : 0.028949039056897163
Loss at batch 70 : 0.010664916597306728
Loss at batch 80 : 0.016740547493100166
Loss at batch 90 : 0.020041288807988167
Loss at batch 100 : 0.014342321082949638
Loss at batch 110 : 0.021718470379710197
Loss at batch 120 : 0.015399397350847721
Loss at batch 130 : 0.013949028216302395
Loss at batch 140 : 0.01118073146790266
Loss at batch 150 : 0.03139224275946617
Loss at batch 160 : 0.010625429451465607
Loss at batch 170 : 0.015541494823992252
Loss at batch 180 : 0.020101286470890045
Loss at batch 190 : 0.012907635420560837
Loss at batch 200 : 0.01979244314134121
Loss at batch 210 : 0.009191237390041351
Loss at batch 220 : 0.010799174197018147
Loss at batch 230 : 0.01303500309586525
Loss at batch 240 : 0.014672769233584404
Loss at batch 250 : 0.02108493074774742
Loss at batch 260 : 0.005543519742786884
Loss at batch 270 : 0.012981841340661049
Loss at batch 280 : 0.02110767923295498
Loss at batch 290 : 0.015606414526700974
Loss at batch 300 : 0.015017585828900337
Loss at batch 310 : 0.008613411337137222
Loss at batch 320 : 0.011098616756498814
Loss at batch 330 : 0.011419816873967648
Loss at batch 340 : 0.02212291955947876
Loss at batch 350 : 0.011810951866209507
Loss at batch 360 : 0.017773034051060677
Loss at batch 370 : 0.025190500542521477
epoch30 finished!
Loss at batch 10 : 0.014214865863323212
Loss at batch 20 : 0.01640241965651512
Loss at batch 30 : 0.01320378016680479
Loss at batch 40 : 0.010703572072088718
Loss at batch 50 : 0.008903631009161472
Loss at batch 60 : 0.010651690885424614
Loss at batch 70 : 0.020254870876669884
Loss at batch 80 : 0.015383404679596424
Loss at batch 90 : 0.0282105915248394
Loss at batch 100 : 0.023051610216498375
Loss at batch 110 : 0.017573419958353043
Loss at batch 120 : 0.014904721640050411
Loss at batch 130 : 0.01922699436545372
Loss at batch 140 : 0.014217403717339039
Loss at batch 150 : 0.021327141672372818
Loss at batch 160 : 0.024992231279611588
Loss at batch 170 : 0.011145728640258312
Loss at batch 180 : 0.024039097130298615
Loss at batch 190 : 0.011265423148870468
Loss at batch 200 : 0.00978258065879345
Loss at batch 210 : 0.014848007820546627
Loss at batch 220 : 0.010815894231200218
Loss at batch 230 : 0.018005914986133575
Loss at batch 240 : 0.011921963654458523
Loss at batch 250 : 0.02604726515710354
Loss at batch 260 : 0.01657320186495781
Loss at batch 270 : 0.01346446480602026
Loss at batch 280 : 0.016871780157089233
Loss at batch 290 : 0.018307067453861237
Loss at batch 300 : 0.01771008037030697
Loss at batch 310 : 0.024197913706302643
Loss at batch 320 : 0.010334420017898083
Loss at batch 330 : 0.00967180822044611
Loss at batch 340 : 0.010551681742072105
Loss at batch 350 : 0.012803851626813412
Loss at batch 360 : 0.01334649883210659
Loss at batch 370 : 0.012129828333854675
epoch31 finished!
Loss at batch 10 : 0.011590663343667984
Loss at batch 20 : 0.025266792625188828
Loss at batch 30 : 0.01715810038149357
Loss at batch 40 : 0.013150160200893879
Loss at batch 50 : 0.01605316624045372
Loss at batch 60 : 0.01202696468681097
Loss at batch 70 : 0.012199332937598228
Loss at batch 80 : 0.013813079334795475
Loss at batch 90 : 0.009833640418946743
Loss at batch 100 : 0.01859787106513977
Loss at batch 110 : 0.020850909873843193
Loss at batch 120 : 0.01484082080423832
Loss at batch 130 : 0.01599181443452835
Loss at batch 140 : 0.019092142581939697
Loss at batch 150 : 0.026709862053394318
Loss at batch 160 : 0.006650485098361969
Loss at batch 170 : 0.017077354714274406
Loss at batch 180 : 0.01988297887146473
Loss at batch 190 : 0.00853589829057455
Loss at batch 200 : 0.019709153100848198
Loss at batch 210 : 0.011245555244386196
Loss at batch 220 : 0.028579367324709892
Loss at batch 230 : 0.012464177794754505
Loss at batch 240 : 0.018426615744829178
Loss at batch 250 : 0.019280366599559784
Loss at batch 260 : 0.011620464734733105
Loss at batch 270 : 0.011936363764107227
Loss at batch 280 : 0.019864842295646667
Loss at batch 290 : 0.011971626430749893
Loss at batch 300 : 0.024957846850156784
Loss at batch 310 : 0.015896430239081383
Loss at batch 320 : 0.012203013524413109
Loss at batch 330 : 0.010024243034422398
Loss at batch 340 : 0.017844200134277344
Loss at batch 350 : 0.011179211549460888
Loss at batch 360 : 0.01322090346366167
Loss at batch 370 : 0.01580996811389923
epoch32 finished!
Loss at batch 10 : 0.011959441937506199
Loss at batch 20 : 0.020227722823619843
Loss at batch 30 : 0.006922196596860886
Loss at batch 40 : 0.02687571756541729
Loss at batch 50 : 0.005712588783353567
Loss at batch 60 : 0.007629601284861565
Loss at batch 70 : 0.024969931691884995
Loss at batch 80 : 0.019953414797782898
Loss at batch 90 : 0.010043374262750149
Loss at batch 100 : 0.02869625762104988
Loss at batch 110 : 0.016553549095988274
Loss at batch 120 : 0.017268972471356392
Loss at batch 130 : 0.020231548696756363
Loss at batch 140 : 0.015564311295747757
Loss at batch 150 : 0.02425478585064411
Loss at batch 160 : 0.014010070823132992
Loss at batch 170 : 0.01891437917947769
Loss at batch 180 : 0.015292609110474586
Loss at batch 190 : 0.02374381572008133
Loss at batch 200 : 0.011321241967380047
Loss at batch 210 : 0.01646469533443451
Loss at batch 220 : 0.009687012061476707
Loss at batch 230 : 0.010439210571348667
Loss at batch 240 : 0.012321331538259983
Loss at batch 250 : 0.014037054032087326
Loss at batch 260 : 0.013864077627658844
Loss at batch 270 : 0.01987982727587223
Loss at batch 280 : 0.010240592062473297
Loss at batch 290 : 0.014182507991790771
Loss at batch 300 : 0.010958606377243996
Loss at batch 310 : 0.00998175423592329
Loss at batch 320 : 0.030109433457255363
Loss at batch 330 : 0.02097952552139759
Loss at batch 340 : 0.024852978065609932
Loss at batch 350 : 0.021903332322835922
Loss at batch 360 : 0.010321223177015781
Loss at batch 370 : 0.017672669142484665
epoch33 finished!
Loss at batch 10 : 0.01628485880792141
Loss at batch 20 : 0.02129136398434639
Loss at batch 30 : 0.015174760483205318
Loss at batch 40 : 0.026596112176775932
Loss at batch 50 : 0.019271181896328926
Loss at batch 60 : 0.02196967788040638
Loss at batch 70 : 0.022031698375940323
Loss at batch 80 : 0.019632644951343536
Loss at batch 90 : 0.015118962153792381
Loss at batch 100 : 0.023263245820999146
Loss at batch 110 : 0.021185658872127533
Loss at batch 120 : 0.019533535465598106
Loss at batch 130 : 0.012120847590267658
Loss at batch 140 : 0.01203539315611124
Loss at batch 150 : 0.016391858458518982
Loss at batch 160 : 0.010887018404901028
Loss at batch 170 : 0.017374452203512192
Loss at batch 180 : 0.015350990928709507
Loss at batch 190 : 0.00938583817332983
Loss at batch 200 : 0.01940166763961315
Loss at batch 210 : 0.013313276693224907
Loss at batch 220 : 0.02081475220620632
Loss at batch 230 : 0.013677551411092281
Loss at batch 240 : 0.01713516190648079
Loss at batch 250 : 0.009897411800920963
Loss at batch 260 : 0.013167639262974262
Loss at batch 270 : 0.014846263453364372
Loss at batch 280 : 0.01902928575873375
Loss at batch 290 : 0.014434264041483402
Loss at batch 300 : 0.021649353206157684
Loss at batch 310 : 0.020161405205726624
Loss at batch 320 : 0.012499521486461163
Loss at batch 330 : 0.021640710532665253
Loss at batch 340 : 0.010574371553957462
Loss at batch 350 : 0.011949914507567883
Loss at batch 360 : 0.03372766822576523
Loss at batch 370 : 0.011314005590975285
epoch34 finished!
Loss at batch 10 : 0.01126288715749979
Loss at batch 20 : 0.009878241457045078
Loss at batch 30 : 0.0188396368175745
Loss at batch 40 : 0.018843425437808037
Loss at batch 50 : 0.02434161864221096
Loss at batch 60 : 0.010918769985437393
Loss at batch 70 : 0.01569347083568573
Loss at batch 80 : 0.010849446058273315
Loss at batch 90 : 0.010584532283246517
Loss at batch 100 : 0.010165449231863022
Loss at batch 110 : 0.01964140497148037
Loss at batch 120 : 0.018060782924294472
Loss at batch 130 : 0.014436552301049232
Loss at batch 140 : 0.02625073306262493
Loss at batch 150 : 0.012917838990688324
Loss at batch 160 : 0.016350150108337402
Loss at batch 170 : 0.009861528873443604
Loss at batch 180 : 0.010499139316380024
Loss at batch 190 : 0.014681274071335793
Loss at batch 200 : 0.009444118477404118
Loss at batch 210 : 0.015888607129454613
Loss at batch 220 : 0.012392453849315643
Loss at batch 230 : 0.006530581507831812
Loss at batch 240 : 0.017877431586384773
Loss at batch 250 : 0.01609124056994915
Loss at batch 260 : 0.01326038409024477
Loss at batch 270 : 0.017676083371043205
Loss at batch 280 : 0.013451695442199707
Loss at batch 290 : 0.012462989427149296
Loss at batch 300 : 0.016868969425559044
Loss at batch 310 : 0.012818512506783009
Loss at batch 320 : 0.02584577351808548
Loss at batch 330 : 0.017704064026474953
Loss at batch 340 : 0.017732195556163788
Loss at batch 350 : 0.008326005190610886
Loss at batch 360 : 0.009008646942675114
Loss at batch 370 : 0.007985326461493969
epoch35 finished!
Loss at batch 10 : 0.017544345930218697
Loss at batch 20 : 0.01061167847365141
Loss at batch 30 : 0.015929177403450012
Loss at batch 40 : 0.022484252229332924
Loss at batch 50 : 0.008618620224297047
Loss at batch 60 : 0.006992051377892494
Loss at batch 70 : 0.021979447454214096
Loss at batch 80 : 0.009518399834632874
Loss at batch 90 : 0.020922021940350533
Loss at batch 100 : 0.018785934895277023
Loss at batch 110 : 0.020259244367480278
Loss at batch 120 : 0.016197238117456436
Loss at batch 130 : 0.015406642109155655
Loss at batch 140 : 0.01135191135108471
Loss at batch 150 : 0.02078557386994362
Loss at batch 160 : 0.008653118275105953
Loss at batch 170 : 0.03331143409013748
Loss at batch 180 : 0.02238890342414379
Loss at batch 190 : 0.010363929904997349
Loss at batch 200 : 0.016649149358272552
Loss at batch 210 : 0.02130332961678505
Loss at batch 220 : 0.016754789277911186
Loss at batch 230 : 0.012742391787469387
Loss at batch 240 : 0.018749382346868515
Loss at batch 250 : 0.009828325361013412
Loss at batch 260 : 0.01258056703954935
Loss at batch 270 : 0.014827593229711056
Loss at batch 280 : 0.024225721135735512
Loss at batch 290 : 0.016225822269916534
Loss at batch 300 : 0.014761054888367653
Loss at batch 310 : 0.015566728077828884
Loss at batch 320 : 0.020148150622844696
Loss at batch 330 : 0.018119342625141144
Loss at batch 340 : 0.021630825474858284
Loss at batch 350 : 0.024219539016485214
Loss at batch 360 : 0.013104559853672981
Loss at batch 370 : 0.011297622695565224
epoch36 finished!
Loss at batch 10 : 0.010354635305702686
Loss at batch 20 : 0.01245570182800293
Loss at batch 30 : 0.01680714823305607
Loss at batch 40 : 0.017963077872991562
Loss at batch 50 : 0.01282498985528946
Loss at batch 60 : 0.029500262811779976
Loss at batch 70 : 0.016391975805163383
Loss at batch 80 : 0.015299048274755478
Loss at batch 90 : 0.014521396718919277
Loss at batch 100 : 0.006266563665121794
Loss at batch 110 : 0.018579168245196342
Loss at batch 120 : 0.00990854948759079
Loss at batch 130 : 0.020375141873955727
Loss at batch 140 : 0.013333759270608425
Loss at batch 150 : 0.013954470865428448
Loss at batch 160 : 0.00759075628593564
Loss at batch 170 : 0.01772906817495823
Loss at batch 180 : 0.010580689646303654
Loss at batch 190 : 0.010516036301851273
Loss at batch 200 : 0.02109741047024727
Loss at batch 210 : 0.009724738076329231
Loss at batch 220 : 0.009233401156961918
Loss at batch 230 : 0.011931877583265305
Loss at batch 240 : 0.018701955676078796
Loss at batch 250 : 0.009581254795193672
Loss at batch 260 : 0.011398944072425365
Loss at batch 270 : 0.005605787970125675
Loss at batch 280 : 0.008973225951194763
Loss at batch 290 : 0.022258499637246132
Loss at batch 300 : 0.019785264506936073
Loss at batch 310 : 0.013131773099303246
Loss at batch 320 : 0.021072784438729286
Loss at batch 330 : 0.009015889838337898
Loss at batch 340 : 0.017943624407052994
Loss at batch 350 : 0.016737503930926323
Loss at batch 360 : 0.008509593084454536
Loss at batch 370 : 0.017848744988441467
epoch37 finished!
Loss at batch 10 : 0.010889398865401745
Loss at batch 20 : 0.010671746917068958
Loss at batch 30 : 0.011275197379291058
Loss at batch 40 : 0.018738068640232086
Loss at batch 50 : 0.016107451170682907
Loss at batch 60 : 0.015268910676240921
Loss at batch 70 : 0.01151321455836296
Loss at batch 80 : 0.021991778165102005
Loss at batch 90 : 0.012927984818816185
Loss at batch 100 : 0.01764720492064953
Loss at batch 110 : 0.012155544012784958
Loss at batch 120 : 0.010066473856568336
Loss at batch 130 : 0.011065109632909298
Loss at batch 140 : 0.007755886297672987
Loss at batch 150 : 0.014870873652398586
Loss at batch 160 : 0.014513486996293068
Loss at batch 170 : 0.014253477565944195
Loss at batch 180 : 0.02428855374455452
Loss at batch 190 : 0.019229497760534286
Loss at batch 200 : 0.013303332962095737
Loss at batch 210 : 0.014339403249323368
Loss at batch 220 : 0.010800134390592575
Loss at batch 230 : 0.012554760091006756
Loss at batch 240 : 0.01584109477698803
Loss at batch 250 : 0.012010762467980385
Loss at batch 260 : 0.006761057302355766
Loss at batch 270 : 0.021434539929032326
Loss at batch 280 : 0.01960631273686886
Loss at batch 290 : 0.017437756061553955
Loss at batch 300 : 0.019365018233656883
Loss at batch 310 : 0.01584002375602722
Loss at batch 320 : 0.02229839377105236
Loss at batch 330 : 0.01174227986484766
Loss at batch 340 : 0.011967589147388935
Loss at batch 350 : 0.011541327461600304
Loss at batch 360 : 0.01161122415214777
Loss at batch 370 : 0.010531380772590637
epoch38 finished!
Loss at batch 10 : 0.019102714955806732
Loss at batch 20 : 0.012858308851718903
Loss at batch 30 : 0.0245630145072937
Loss at batch 40 : 0.012159041129052639
Loss at batch 50 : 0.019054682925343513
Loss at batch 60 : 0.020877636969089508
Loss at batch 70 : 0.007807298097759485
Loss at batch 80 : 0.013792489655315876
Loss at batch 90 : 0.02051873877644539
Loss at batch 100 : 0.027392210438847542
Loss at batch 110 : 0.008749481290578842
Loss at batch 120 : 0.01804572343826294
Loss at batch 130 : 0.019594039767980576
Loss at batch 140 : 0.017212124541401863
Loss at batch 150 : 0.019035471603274345
Loss at batch 160 : 0.02546943537890911
Loss at batch 170 : 0.012178243137896061
Loss at batch 180 : 0.011727815493941307
Loss at batch 190 : 0.011743675917387009
Loss at batch 200 : 0.019423963502049446
Loss at batch 210 : 0.017893485724925995
Loss at batch 220 : 0.018025219440460205
Loss at batch 230 : 0.021850742399692535
Loss at batch 240 : 0.02126174420118332
Loss at batch 250 : 0.012340196408331394
Loss at batch 260 : 0.008300305344164371
Loss at batch 270 : 0.02466261200606823
Loss at batch 280 : 0.046809617429971695
Loss at batch 290 : 0.011355705559253693
Loss at batch 300 : 0.015743091702461243
Loss at batch 310 : 0.026667915284633636
Loss at batch 320 : 0.014850277453660965
Loss at batch 330 : 0.009708791971206665
Loss at batch 340 : 0.019268786534667015
Loss at batch 350 : 0.00894623901695013
Loss at batch 360 : 0.014085150323808193
Loss at batch 370 : 0.016754746437072754
epoch39 finished!
Loss at batch 10 : 0.0163638424128294
Loss at batch 20 : 0.007114680949598551
Loss at batch 30 : 0.009580173529684544
Loss at batch 40 : 0.015440220944583416
Loss at batch 50 : 0.01239422895014286
Loss at batch 60 : 0.015672463923692703
Loss at batch 70 : 0.014352898113429546
Loss at batch 80 : 0.010617478750646114
Loss at batch 90 : 0.018815621733665466
Loss at batch 100 : 0.018259964883327484
Loss at batch 110 : 0.011818568222224712
Loss at batch 120 : 0.011707521043717861
Loss at batch 130 : 0.0125034861266613
Loss at batch 140 : 0.0110248364508152
Loss at batch 150 : 0.01938331313431263
Loss at batch 160 : 0.015741974115371704
Loss at batch 170 : 0.009661829099059105
Loss at batch 180 : 0.017781034111976624
Loss at batch 190 : 0.013135313056409359
Loss at batch 200 : 0.01731840893626213
Loss at batch 210 : 0.011031393893063068
Loss at batch 220 : 0.007766562048345804
Loss at batch 230 : 0.023971786722540855
Loss at batch 240 : 0.009618496522307396
Loss at batch 250 : 0.012574021704494953
Loss at batch 260 : 0.014058046042919159
Loss at batch 270 : 0.021687794476747513
Loss at batch 280 : 0.02122468315064907
Loss at batch 290 : 0.02007560059428215
Loss at batch 300 : 0.00874418392777443
Loss at batch 310 : 0.012941352091729641
Loss at batch 320 : 0.018408097326755524
Loss at batch 330 : 0.021786104887723923
Loss at batch 340 : 0.027075080201029778
Loss at batch 350 : 0.019157739356160164
Loss at batch 360 : 0.016961896792054176
Loss at batch 370 : 0.01691201515495777
epoch40 finished!
Loss at batch 10 : 0.00634902436286211
Loss at batch 20 : 0.011070084758102894
Loss at batch 30 : 0.024672992527484894
Loss at batch 40 : 0.018333392217755318
Loss at batch 50 : 0.01170408446341753
Loss at batch 60 : 0.01803857833147049
Loss at batch 70 : 0.014170819893479347
Loss at batch 80 : 0.012069645337760448
Loss at batch 90 : 0.01487408485263586
Loss at batch 100 : 0.020438451319932938
Loss at batch 110 : 0.013489860109984875
Loss at batch 120 : 0.01005531381815672
Loss at batch 130 : 0.0141007574275136
Loss at batch 140 : 0.03165928274393082
Loss at batch 150 : 0.020058009773492813
Loss at batch 160 : 0.017129898071289062
Loss at batch 170 : 0.0096587548032403
Loss at batch 180 : 0.014730705879628658
Loss at batch 190 : 0.007493660319596529
Loss at batch 200 : 0.010775287635624409
Loss at batch 210 : 0.010856463573873043
Loss at batch 220 : 0.02294980362057686
Loss at batch 230 : 0.012753404676914215
Loss at batch 240 : 0.0191416684538126
Loss at batch 250 : 0.01204332523047924
Loss at batch 260 : 0.016008460894227028
Loss at batch 270 : 0.012535610236227512
Loss at batch 280 : 0.0224673580378294
Loss at batch 290 : 0.02014833316206932
Loss at batch 300 : 0.015782350674271584
Loss at batch 310 : 0.011293378658592701
Loss at batch 320 : 0.0293528251349926
Loss at batch 330 : 0.010719564743340015
Loss at batch 340 : 0.02327219769358635
Loss at batch 350 : 0.011470661498606205
Loss at batch 360 : 0.033750053495168686
Loss at batch 370 : 0.016741137951612473
epoch41 finished!
Loss at batch 10 : 0.014066725969314575
Loss at batch 20 : 0.010714362375438213
Loss at batch 30 : 0.011992637068033218
Loss at batch 40 : 0.01217228639870882
Loss at batch 50 : 0.011882731691002846
Loss at batch 60 : 0.012513074092566967
Loss at batch 70 : 0.01854572258889675
Loss at batch 80 : 0.015924368053674698
Loss at batch 90 : 0.007440481334924698
Loss at batch 100 : 0.014800261706113815
Loss at batch 110 : 0.014170452952384949
Loss at batch 120 : 0.0076715461909770966
Loss at batch 130 : 0.014066903851926327
Loss at batch 140 : 0.009781033731997013
Loss at batch 150 : 0.01765100285410881
Loss at batch 160 : 0.01580018363893032
Loss at batch 170 : 0.012918953783810139
Loss at batch 180 : 0.016468876972794533
Loss at batch 190 : 0.01831776648759842
Loss at batch 200 : 0.011044943705201149
Loss at batch 210 : 0.02087285742163658
Loss at batch 220 : 0.00901456642895937
Loss at batch 230 : 0.022401263937354088
Loss at batch 240 : 0.021568145602941513
Loss at batch 250 : 0.010420192033052444
Loss at batch 260 : 0.01897449977695942
Loss at batch 270 : 0.024425974115729332
Loss at batch 280 : 0.02160780131816864
Loss at batch 290 : 0.019884541630744934
Loss at batch 300 : 0.014169483445584774
Loss at batch 310 : 0.008622948080301285
Loss at batch 320 : 0.018068552017211914
Loss at batch 330 : 0.010996500961482525
Loss at batch 340 : 0.015866663306951523
Loss at batch 350 : 0.013968318700790405
Loss at batch 360 : 0.014769298024475574
Loss at batch 370 : 0.01669464074075222
epoch42 finished!
Loss at batch 10 : 0.0077061401680111885
Loss at batch 20 : 0.013276719488203526
Loss at batch 30 : 0.007194869220256805
Loss at batch 40 : 0.009135368280112743
Loss at batch 50 : 0.020341821014881134
Loss at batch 60 : 0.01411565113812685
Loss at batch 70 : 0.01100024115294218
Loss at batch 80 : 0.01873415894806385
Loss at batch 90 : 0.01782599650323391
Loss at batch 100 : 0.014339104294776917
Loss at batch 110 : 0.010587882250547409
Loss at batch 120 : 0.01752334274351597
Loss at batch 130 : 0.02555137500166893
Loss at batch 140 : 0.014673732221126556
Loss at batch 150 : 0.013590548187494278
Loss at batch 160 : 0.018932558596134186
Loss at batch 170 : 0.01432702038437128
Loss at batch 180 : 0.011657068505883217
Loss at batch 190 : 0.02028936706483364
Loss at batch 200 : 0.006561685353517532
Loss at batch 210 : 0.01388112735003233
Loss at batch 220 : 0.006980158854275942
Loss at batch 230 : 0.02206694334745407
Loss at batch 240 : 0.00640904949977994
Loss at batch 250 : 0.012283374555408955
Loss at batch 260 : 0.017166493460536003
Loss at batch 270 : 0.011852890253067017
Loss at batch 280 : 0.014482255093753338
Loss at batch 290 : 0.01829429529607296
Loss at batch 300 : 0.013913021422922611
Loss at batch 310 : 0.010145614854991436
Loss at batch 320 : 0.013394815847277641
Loss at batch 330 : 0.01543012447655201
Loss at batch 340 : 0.029470017179846764
Loss at batch 350 : 0.02017519436776638
Loss at batch 360 : 0.01094445213675499
Loss at batch 370 : 0.015597911551594734
epoch43 finished!
Loss at batch 10 : 0.014843566343188286
Loss at batch 20 : 0.015341799706220627
Loss at batch 30 : 0.01221194863319397
Loss at batch 40 : 0.013133215717971325
Loss at batch 50 : 0.014852877706289291
Loss at batch 60 : 0.01502761896699667
Loss at batch 70 : 0.013615116477012634
Loss at batch 80 : 0.014591991901397705
Loss at batch 90 : 0.014173125848174095
Loss at batch 100 : 0.01725762151181698
Loss at batch 110 : 0.016913041472434998
Loss at batch 120 : 0.017345141619443893
Loss at batch 130 : 0.018689818680286407
Loss at batch 140 : 0.014252214692533016
Loss at batch 150 : 0.018203144893050194
Loss at batch 160 : 0.00737924687564373
Loss at batch 170 : 0.025573227554559708
Loss at batch 180 : 0.01927126944065094
Loss at batch 190 : 0.019192352890968323
Loss at batch 200 : 0.013858526013791561
Loss at batch 210 : 0.013894871808588505
Loss at batch 220 : 0.010982049629092216
Loss at batch 230 : 0.010821730829775333
Loss at batch 240 : 0.01092817634344101
Loss at batch 250 : 0.021999966353178024
Loss at batch 260 : 0.01028425432741642
Loss at batch 270 : 0.008819415234029293
Loss at batch 280 : 0.007219781167805195
Loss at batch 290 : 0.013072668574750423
Loss at batch 300 : 0.013491997495293617
Loss at batch 310 : 0.013706321828067303
Loss at batch 320 : 0.010022939182817936
Loss at batch 330 : 0.009423031471669674
Loss at batch 340 : 0.017616618424654007
Loss at batch 350 : 0.015321153216063976
Loss at batch 360 : 0.025152355432510376
Loss at batch 370 : 0.020981252193450928
epoch44 finished!
Loss at batch 10 : 0.01532310713082552
Loss at batch 20 : 0.019139552488923073
Loss at batch 30 : 0.021822113543748856
Loss at batch 40 : 0.01731828786432743
Loss at batch 50 : 0.01010981947183609
Loss at batch 60 : 0.022871578112244606
Loss at batch 70 : 0.019657567143440247
Loss at batch 80 : 0.013197244144976139
Loss at batch 90 : 0.009697982110083103
Loss at batch 100 : 0.018412506207823753
Loss at batch 110 : 0.018185213208198547
Loss at batch 120 : 0.01310085877776146
Loss at batch 130 : 0.011666095815598965
Loss at batch 140 : 0.009932096116244793
Loss at batch 150 : 0.008667170070111752
Loss at batch 160 : 0.02059459127485752
Loss at batch 170 : 0.01837763376533985
Loss at batch 180 : 0.018368961289525032
Loss at batch 190 : 0.013342722319066525
Loss at batch 200 : 0.008771924301981926
Loss at batch 210 : 0.009336323477327824
Loss at batch 220 : 0.011560444720089436
Loss at batch 230 : 0.012526891194283962
Loss at batch 240 : 0.020728398114442825
Loss at batch 250 : 0.01869269087910652
Loss at batch 260 : 0.014803755097091198
Loss at batch 270 : 0.013263190165162086
Loss at batch 280 : 0.020873714238405228
Loss at batch 290 : 0.015795521438121796
Loss at batch 300 : 0.020244650542736053
Loss at batch 310 : 0.011616119183599949
Loss at batch 320 : 0.011214576661586761
Loss at batch 330 : 0.021723464131355286
Loss at batch 340 : 0.014015451073646545
Loss at batch 350 : 0.014060850255191326
Loss at batch 360 : 0.02222798764705658
Loss at batch 370 : 0.01923735812306404
epoch45 finished!
Loss at batch 10 : 0.009545761160552502
Loss at batch 20 : 0.011055477894842625
Loss at batch 30 : 0.009119585156440735
Loss at batch 40 : 0.030052483081817627
Loss at batch 50 : 0.02155151031911373
Loss at batch 60 : 0.009757916443049908
Loss at batch 70 : 0.01771511696279049
Loss at batch 80 : 0.016180995851755142
Loss at batch 90 : 0.007058010436594486
Loss at batch 100 : 0.014065109193325043
Loss at batch 110 : 0.014084375463426113
Loss at batch 120 : 0.014860855415463448
Loss at batch 130 : 0.01601298525929451
Loss at batch 140 : 0.010591469705104828
Loss at batch 150 : 0.013151002116501331
Loss at batch 160 : 0.008394510485231876
Loss at batch 170 : 0.014557529240846634
Loss at batch 180 : 0.010595791973173618
Loss at batch 190 : 0.007288398686796427
Loss at batch 200 : 0.01502358540892601
Loss at batch 210 : 0.02011251077055931
Loss at batch 220 : 0.009111493825912476
Loss at batch 230 : 0.016572602093219757
Loss at batch 240 : 0.01861981302499771
Loss at batch 250 : 0.03170442581176758
Loss at batch 260 : 0.01492519024759531
Loss at batch 270 : 0.013666960410773754
Loss at batch 280 : 0.013290423899888992
Loss at batch 290 : 0.010159681551158428
Loss at batch 300 : 0.015379342250525951
Loss at batch 310 : 0.021347979083657265
Loss at batch 320 : 0.006286236457526684
Loss at batch 330 : 0.015584600158035755
Loss at batch 340 : 0.013750557787716389
Loss at batch 350 : 0.026134973391890526
Loss at batch 360 : 0.021977176889777184
Loss at batch 370 : 0.009858090430498123
epoch46 finished!
Loss at batch 10 : 0.0174432210624218
Loss at batch 20 : 0.008555236272513866
Loss at batch 30 : 0.011332855559885502
Loss at batch 40 : 0.009294357150793076
Loss at batch 50 : 0.01594754308462143
Loss at batch 60 : 0.016941087320446968
Loss at batch 70 : 0.022904332727193832
Loss at batch 80 : 0.011086433194577694
Loss at batch 90 : 0.012406152673065662
Loss at batch 100 : 0.010272622108459473
Loss at batch 110 : 0.017638156190514565
Loss at batch 120 : 0.01267982181161642
Loss at batch 130 : 0.03054753690958023
Loss at batch 140 : 0.01125528384000063
Loss at batch 150 : 0.009655321016907692
Loss at batch 160 : 0.0092428307980299
Loss at batch 170 : 0.012684700079262257
Loss at batch 180 : 0.01639002375304699
Loss at batch 190 : 0.012942777946591377
Loss at batch 200 : 0.008294185623526573
Loss at batch 210 : 0.010938229039311409
Loss at batch 220 : 0.011179394088685513
Loss at batch 230 : 0.007861019112169743
Loss at batch 240 : 0.009955232962965965
Loss at batch 250 : 0.014994685538113117
Loss at batch 260 : 0.008857724256813526
Loss at batch 270 : 0.02228762023150921
Loss at batch 280 : 0.014592289924621582
Loss at batch 290 : 0.016471661627292633
Loss at batch 300 : 0.010631920769810677
Loss at batch 310 : 0.014864721335470676
Loss at batch 320 : 0.01436892431229353
Loss at batch 330 : 0.01792215369641781
Loss at batch 340 : 0.014220432378351688
Loss at batch 350 : 0.012871750630438328
Loss at batch 360 : 0.019217582419514656
Loss at batch 370 : 0.010086767375469208
epoch47 finished!
Loss at batch 10 : 0.01595013588666916
Loss at batch 20 : 0.013828235678374767
Loss at batch 30 : 0.02139047160744667
Loss at batch 40 : 0.022083507850766182
Loss at batch 50 : 0.02203967608511448
Loss at batch 60 : 0.012198833748698235
Loss at batch 70 : 0.01489353459328413
Loss at batch 80 : 0.007845253683626652
Loss at batch 90 : 0.011376927606761456
Loss at batch 100 : 0.007713998667895794
Loss at batch 110 : 0.012522028759121895
Loss at batch 120 : 0.00978580117225647
Loss at batch 130 : 0.02949904277920723
Loss at batch 140 : 0.009720847941935062
Loss at batch 150 : 0.021237671375274658
Loss at batch 160 : 0.007925662212073803
Loss at batch 170 : 0.02327435463666916
Loss at batch 180 : 0.015343292616307735
Loss at batch 190 : 0.01818419061601162
Loss at batch 200 : 0.016865232959389687
Loss at batch 210 : 0.014508014544844627
Loss at batch 220 : 0.012932523153722286
Loss at batch 230 : 0.014100916683673859
Loss at batch 240 : 0.01643345132470131
Loss at batch 250 : 0.009618682786822319
Loss at batch 260 : 0.010348725132644176
Loss at batch 270 : 0.018046734854578972
Loss at batch 280 : 0.01753847859799862
Loss at batch 290 : 0.014914881438016891
Loss at batch 300 : 0.010404599830508232
Loss at batch 310 : 0.012919530272483826
Loss at batch 320 : 0.013696969486773014
Loss at batch 330 : 0.01067105308175087
Loss at batch 340 : 0.021743079647421837
Loss at batch 350 : 0.012453560717403889
Loss at batch 360 : 0.00828368030488491
Loss at batch 370 : 0.016659842804074287
epoch48 finished!
Loss at batch 10 : 0.009595141746103764
Loss at batch 20 : 0.01480298861861229
Loss at batch 30 : 0.02068411000072956
Loss at batch 40 : 0.007769382558763027
Loss at batch 50 : 0.016135232523083687
Loss at batch 60 : 0.016100075095891953
Loss at batch 70 : 0.010467009618878365
Loss at batch 80 : 0.014823305420577526
Loss at batch 90 : 0.01519744098186493
Loss at batch 100 : 0.01379342470318079
Loss at batch 110 : 0.021118896082043648
Loss at batch 120 : 0.026746192947030067
Loss at batch 130 : 0.01424593199044466
Loss at batch 140 : 0.03604542464017868
Loss at batch 150 : 0.012963141314685345
Loss at batch 160 : 0.015153709799051285
Loss at batch 170 : 0.02089313603937626
Loss at batch 180 : 0.01041419804096222
Loss at batch 190 : 0.007863779552280903
Loss at batch 200 : 0.02253246121108532
Loss at batch 210 : 0.021821191534399986
Loss at batch 220 : 0.009831204079091549
Loss at batch 230 : 0.01929312013089657
Loss at batch 240 : 0.03917554393410683
Loss at batch 250 : 0.013132074847817421
Loss at batch 260 : 0.020889105275273323
Loss at batch 270 : 0.019111892208456993
Loss at batch 280 : 0.022780999541282654
Loss at batch 290 : 0.010358487255871296
Loss at batch 300 : 0.014861702919006348
Loss at batch 310 : 0.015524487011134624
Loss at batch 320 : 0.006743375677615404
Loss at batch 330 : 0.017559295520186424
Loss at batch 340 : 0.009535999968647957
Loss at batch 350 : 0.01438065804541111
Loss at batch 360 : 0.01030692644417286
Loss at batch 370 : 0.030743125826120377
epoch49 finished!
Loss at batch 10 : 0.013138129375874996
Loss at batch 20 : 0.011539271101355553
Loss at batch 30 : 0.01744944602251053
Loss at batch 40 : 0.017924131825566292
Loss at batch 50 : 0.025823509320616722
Loss at batch 60 : 0.013196714222431183
Loss at batch 70 : 0.007557686883956194
Loss at batch 80 : 0.015091749839484692
Loss at batch 90 : 0.011055747047066689
Loss at batch 100 : 0.011611368507146835
Loss at batch 110 : 0.014154063537716866
Loss at batch 120 : 0.02457193285226822
Loss at batch 130 : 0.014660079032182693
Loss at batch 140 : 0.02028999663889408
Loss at batch 150 : 0.011917943134903908
Loss at batch 160 : 0.006416134536266327
Loss at batch 170 : 0.014720887877047062
Loss at batch 180 : 0.016150321811437607
Loss at batch 190 : 0.024174174293875694
Loss at batch 200 : 0.014041907154023647
Loss at batch 210 : 0.028004582971334457
Loss at batch 220 : 0.018238598480820656
Loss at batch 230 : 0.01642531529068947
Loss at batch 240 : 0.01472451165318489
Loss at batch 250 : 0.014669456519186497
Loss at batch 260 : 0.01515243761241436
Loss at batch 270 : 0.009782075881958008
Loss at batch 280 : 0.009199130348861217
Loss at batch 290 : 0.012833338230848312
Loss at batch 300 : 0.013720384798943996
Loss at batch 310 : 0.01947634480893612
Loss at batch 320 : 0.016317155212163925
Loss at batch 330 : 0.02189156413078308
Loss at batch 340 : 0.013516485691070557
Loss at batch 350 : 0.012213747948408127
Loss at batch 360 : 0.02240908518433571
Loss at batch 370 : 0.012184656225144863
epoch50 finished!
Loss at batch 10 : 0.009444127790629864
Loss at batch 20 : 0.019545534625649452
Loss at batch 30 : 0.01752086728811264
Loss at batch 40 : 0.007694734260439873
Loss at batch 50 : 0.010083193890750408
Loss at batch 60 : 0.015097755938768387
Loss at batch 70 : 0.014822986908257008
Loss at batch 80 : 0.016019966453313828
Loss at batch 90 : 0.017071116715669632
Loss at batch 100 : 0.012735861353576183
Loss at batch 110 : 0.010512210428714752
Loss at batch 120 : 0.011697385460138321
Loss at batch 130 : 0.011676008813083172
Loss at batch 140 : 0.010332487523555756
Loss at batch 150 : 0.012084350921213627
Loss at batch 160 : 0.007783497218042612
Loss at batch 170 : 0.01767594926059246
Loss at batch 180 : 0.009046515449881554
Loss at batch 190 : 0.015286089852452278
Loss at batch 200 : 0.009725555777549744
Loss at batch 210 : 0.02290191687643528
Loss at batch 220 : 0.013499648310244083
Loss at batch 230 : 0.007675968110561371
Loss at batch 240 : 0.010051778517663479
Loss at batch 250 : 0.012319578789174557
Loss at batch 260 : 0.017215408384799957
Loss at batch 270 : 0.009343541227281094
Loss at batch 280 : 0.008643810637295246
Loss at batch 290 : 0.015349560417234898
Loss at batch 300 : 0.00975876022130251
Loss at batch 310 : 0.013641741126775742
Loss at batch 320 : 0.014693447388708591
Loss at batch 330 : 0.01385572087019682
Loss at batch 340 : 0.02194337733089924
Loss at batch 350 : 0.012976134195923805
Loss at batch 360 : 0.017640359699726105
Loss at batch 370 : 0.009662370197474957
epoch51 finished!
Loss at batch 10 : 0.018430497497320175
Loss at batch 20 : 0.020095380023121834
Loss at batch 30 : 0.015552284196019173
Loss at batch 40 : 0.012238855473697186
Loss at batch 50 : 0.013694912195205688
Loss at batch 60 : 0.01756117306649685
Loss at batch 70 : 0.012019181624054909
Loss at batch 80 : 0.011872949078679085
Loss at batch 90 : 0.006714462768286467
Loss at batch 100 : 0.013855214230716228
Loss at batch 110 : 0.018377458676695824
Loss at batch 120 : 0.010891388170421124
Loss at batch 130 : 0.013855730183422565
Loss at batch 140 : 0.013174752704799175
Loss at batch 150 : 0.014697767794132233
Loss at batch 160 : 0.011261007748544216
Loss at batch 170 : 0.013660931028425694
Loss at batch 180 : 0.01592046208679676
Loss at batch 190 : 0.013980884104967117
Loss at batch 200 : 0.00967238936573267
Loss at batch 210 : 0.016440052539110184
Loss at batch 220 : 0.021954121068120003
Loss at batch 230 : 0.010014180094003677
Loss at batch 240 : 0.018224729225039482
Loss at batch 250 : 0.02007053792476654
Loss at batch 260 : 0.018137041479349136
Loss at batch 270 : 0.016848748549818993
Loss at batch 280 : 0.015559284016489983
Loss at batch 290 : 0.020496182143688202
Loss at batch 300 : 0.0172696802765131
Loss at batch 310 : 0.01586720161139965
Loss at batch 320 : 0.02856322191655636
Loss at batch 330 : 0.01434206124395132
Loss at batch 340 : 0.012296745553612709
Loss at batch 350 : 0.026380255818367004
Loss at batch 360 : 0.01980016566812992
Loss at batch 370 : 0.014480426907539368
epoch52 finished!
Loss at batch 10 : 0.009848516434431076
Loss at batch 20 : 0.01249356847256422
Loss at batch 30 : 0.018605921417474747
Loss at batch 40 : 0.016639966517686844
Loss at batch 50 : 0.03126278892159462
Loss at batch 60 : 0.014057153835892677
Loss at batch 70 : 0.017525766044855118
Loss at batch 80 : 0.01059325598180294
Loss at batch 90 : 0.021643759682774544
Loss at batch 100 : 0.010842444375157356
Loss at batch 110 : 0.013153978623449802
Loss at batch 120 : 0.01112867146730423
Loss at batch 130 : 0.015017700381577015
Loss at batch 140 : 0.020181898027658463
Loss at batch 150 : 0.018329139798879623
Loss at batch 160 : 0.01425738912075758
Loss at batch 170 : 0.03029937855899334
Loss at batch 180 : 0.014683576300740242
Loss at batch 190 : 0.01618051901459694
Loss at batch 200 : 0.01339581236243248
Loss at batch 210 : 0.017989637330174446
Loss at batch 220 : 0.009870478883385658
Loss at batch 230 : 0.015221530571579933
Loss at batch 240 : 0.010936001315712929
Loss at batch 250 : 0.007842033170163631
Loss at batch 260 : 0.009060880169272423
Loss at batch 270 : 0.014229962602257729
Loss at batch 280 : 0.01400887593626976
Loss at batch 290 : 0.022756969556212425
Loss at batch 300 : 0.017150813713669777
Loss at batch 310 : 0.030455704778432846
Loss at batch 320 : 0.013865196146070957
Loss at batch 330 : 0.027041029185056686
Loss at batch 340 : 0.011751671321690083
Loss at batch 350 : 0.00803069956600666
Loss at batch 360 : 0.0184756089001894
Loss at batch 370 : 0.014626482501626015
epoch53 finished!
Loss at batch 10 : 0.015830354765057564
Loss at batch 20 : 0.011337563395500183
Loss at batch 30 : 0.01082459557801485
Loss at batch 40 : 0.017077872529625893
Loss at batch 50 : 0.016691260039806366
Loss at batch 60 : 0.010584254749119282
Loss at batch 70 : 0.015317031182348728
Loss at batch 80 : 0.014671589247882366
Loss at batch 90 : 0.0106651671230793
Loss at batch 100 : 0.013096131384372711
Loss at batch 110 : 0.024518603459000587
Loss at batch 120 : 0.010537110269069672
Loss at batch 130 : 0.011794771067798138
Loss at batch 140 : 0.019870631396770477
Loss at batch 150 : 0.015266792848706245
Loss at batch 160 : 0.012916473671793938
Loss at batch 170 : 0.031056223437190056
Loss at batch 180 : 0.008339452557265759
Loss at batch 190 : 0.017089322209358215
Loss at batch 200 : 0.01968340203166008
Loss at batch 210 : 0.019812431186437607
Loss at batch 220 : 0.019968681037425995
Loss at batch 230 : 0.014787877909839153
Loss at batch 240 : 0.011301737278699875
Loss at batch 250 : 0.018089497461915016
Loss at batch 260 : 0.016008440405130386
Loss at batch 270 : 0.01753637008368969
Loss at batch 280 : 0.02320585958659649
Loss at batch 290 : 0.0206008180975914
Loss at batch 300 : 0.014275599271059036
Loss at batch 310 : 0.010517962276935577
Loss at batch 320 : 0.01219912525266409
Loss at batch 330 : 0.009583855979144573
Loss at batch 340 : 0.0264383964240551
Loss at batch 350 : 0.011277260258793831
Loss at batch 360 : 0.019911564886569977
Loss at batch 370 : 0.007102811709046364
epoch54 finished!
Loss at batch 10 : 0.018917063251137733
Loss at batch 20 : 0.010756173171103
Loss at batch 30 : 0.012832160107791424
Loss at batch 40 : 0.011777292937040329
Loss at batch 50 : 0.0111544756218791
Loss at batch 60 : 0.010084631852805614
Loss at batch 70 : 0.021470973268151283
Loss at batch 80 : 0.016498150303959846
Loss at batch 90 : 0.01031480636447668
Loss at batch 100 : 0.016462916508316994
Loss at batch 110 : 0.014198444783687592
Loss at batch 120 : 0.009031305089592934
Loss at batch 130 : 0.01458350196480751
Loss at batch 140 : 0.009922422468662262
Loss at batch 150 : 0.01495011430233717
Loss at batch 160 : 0.010761111974716187
Loss at batch 170 : 0.014191186986863613
Loss at batch 180 : 0.008701829239726067
Loss at batch 190 : 0.016265882179141045
Loss at batch 200 : 0.010442160069942474
Loss at batch 210 : 0.016917934641242027
Loss at batch 220 : 0.017926478758454323
Loss at batch 230 : 0.017004938796162605
Loss at batch 240 : 0.012416352517902851
Loss at batch 250 : 0.011743076145648956
Loss at batch 260 : 0.017709936946630478
Loss at batch 270 : 0.01462727040052414
Loss at batch 280 : 0.02678479626774788
Loss at batch 290 : 0.01217567641288042
Loss at batch 300 : 0.01538543589413166
Loss at batch 310 : 0.025032268837094307
Loss at batch 320 : 0.01424015685915947
Loss at batch 330 : 0.014625055715441704
Loss at batch 340 : 0.021045850589871407
Loss at batch 350 : 0.01432547066360712
Loss at batch 360 : 0.016515417024493217
Loss at batch 370 : 0.012627947144210339
epoch55 finished!
Loss at batch 10 : 0.02050255425274372
Loss at batch 20 : 0.016584981232881546
Loss at batch 30 : 0.014181221835315228
Loss at batch 40 : 0.012383876368403435
Loss at batch 50 : 0.01602902263402939
Loss at batch 60 : 0.007276885211467743
Loss at batch 70 : 0.024125108495354652
Loss at batch 80 : 0.008774218149483204
Loss at batch 90 : 0.014788087457418442
Loss at batch 100 : 0.02042960189282894
Loss at batch 110 : 0.015682410448789597
Loss at batch 120 : 0.019238103181123734
Loss at batch 130 : 0.016049794852733612
Loss at batch 140 : 0.008626893162727356
Loss at batch 150 : 0.012623146176338196
Loss at batch 160 : 0.013244039379060268
Loss at batch 170 : 0.02385350689291954
Loss at batch 180 : 0.014898275025188923
Loss at batch 190 : 0.021783998236060143
Loss at batch 200 : 0.017990829423069954
Loss at batch 210 : 0.019047915935516357
Loss at batch 220 : 0.016163647174835205
Loss at batch 230 : 0.01978091336786747
Loss at batch 240 : 0.011817525140941143
Loss at batch 250 : 0.01563269831240177
Loss at batch 260 : 0.01229267567396164
Loss at batch 270 : 0.013965379446744919
Loss at batch 280 : 0.010927767492830753
Loss at batch 290 : 0.018733588978648186
Loss at batch 300 : 0.012851482257246971
Loss at batch 310 : 0.01119188591837883
Loss at batch 320 : 0.02269550785422325
Loss at batch 330 : 0.011874120682477951
Loss at batch 340 : 0.015331181697547436
Loss at batch 350 : 0.01625911146402359
Loss at batch 360 : 0.013885708525776863
Loss at batch 370 : 0.03030717745423317
epoch56 finished!
Loss at batch 10 : 0.010851118713617325
Loss at batch 20 : 0.018849411979317665
Loss at batch 30 : 0.006067091599106789
Loss at batch 40 : 0.017103560268878937
Loss at batch 50 : 0.013872038573026657
Loss at batch 60 : 0.016756732016801834
Loss at batch 70 : 0.01501444075256586
Loss at batch 80 : 0.012843185104429722
Loss at batch 90 : 0.013436101377010345
Loss at batch 100 : 0.01435572374612093
Loss at batch 110 : 0.01791992597281933
Loss at batch 120 : 0.010668712668120861
Loss at batch 130 : 0.009909844025969505
Loss at batch 140 : 0.014259244315326214
Loss at batch 150 : 0.015602595172822475
Loss at batch 160 : 0.009342770092189312
Loss at batch 170 : 0.011918237432837486
Loss at batch 180 : 0.014486279338598251
Loss at batch 190 : 0.00877736322581768
Loss at batch 200 : 0.007681156042963266
Loss at batch 210 : 0.019546501338481903
Loss at batch 220 : 0.017980029806494713
Loss at batch 230 : 0.01636975072324276
Loss at batch 240 : 0.012463639490306377
Loss at batch 250 : 0.006198905408382416
Loss at batch 260 : 0.015304108150303364
Loss at batch 270 : 0.015016797930002213
Loss at batch 280 : 0.020533891394734383
Loss at batch 290 : 0.020155474543571472
Loss at batch 300 : 0.008478213101625443
Loss at batch 310 : 0.014208247885107994
Loss at batch 320 : 0.019425205886363983
Loss at batch 330 : 0.011028071865439415
Loss at batch 340 : 0.011427178047597408
Loss at batch 350 : 0.014405660331249237
Loss at batch 360 : 0.015532581135630608
Loss at batch 370 : 0.017204923555254936
epoch57 finished!
Loss at batch 10 : 0.01095612347126007
Loss at batch 20 : 0.016278544440865517
Loss at batch 30 : 0.015445094555616379
Loss at batch 40 : 0.009057534858584404
Loss at batch 50 : 0.018384866416454315
Loss at batch 60 : 0.014219606295228004
Loss at batch 70 : 0.014016658999025822
Loss at batch 80 : 0.010482902638614178
Loss at batch 90 : 0.019673677161335945
Loss at batch 100 : 0.013187619857490063
Loss at batch 110 : 0.017440302297472954
Loss at batch 120 : 0.015449073165655136
Loss at batch 130 : 0.01656842604279518
Loss at batch 140 : 0.019172171130776405
Loss at batch 150 : 0.011483453214168549
Loss at batch 160 : 0.01906648837029934
Loss at batch 170 : 0.015473577193915844
Loss at batch 180 : 0.018168814480304718
Loss at batch 190 : 0.029000308364629745
Loss at batch 200 : 0.017050383612513542
Loss at batch 210 : 0.010357394814491272
Loss at batch 220 : 0.01825687102973461
Loss at batch 230 : 0.010991908609867096
Loss at batch 240 : 0.0065069375559687614
Loss at batch 250 : 0.00875967275351286
Loss at batch 260 : 0.01456074882298708
Loss at batch 270 : 0.008470689877867699
Loss at batch 280 : 0.01696278527379036
Loss at batch 290 : 0.012412427924573421
Loss at batch 300 : 0.012868902646005154
Loss at batch 310 : 0.015480617061257362
Loss at batch 320 : 0.015351732261478901
Loss at batch 330 : 0.01986018382012844
Loss at batch 340 : 0.014479451812803745
Loss at batch 350 : 0.011736974120140076
Loss at batch 360 : 0.018322648480534554
Loss at batch 370 : 0.012548865750432014
epoch58 finished!
Loss at batch 10 : 0.014338389970362186
Loss at batch 20 : 0.011007588356733322
Loss at batch 30 : 0.010454162955284119
Loss at batch 40 : 0.01003344263881445
Loss at batch 50 : 0.014412954449653625
Loss at batch 60 : 0.01428441796451807
Loss at batch 70 : 0.014534777030348778
Loss at batch 80 : 0.01113204751163721
Loss at batch 90 : 0.01578456535935402
Loss at batch 100 : 0.01992844231426716
Loss at batch 110 : 0.015064788982272148
Loss at batch 120 : 0.025498030707240105
Loss at batch 130 : 0.011384124867618084
Loss at batch 140 : 0.010485170409083366
Loss at batch 150 : 0.02222767286002636
Loss at batch 160 : 0.012000525370240211
Loss at batch 170 : 0.014000799506902695
Loss at batch 180 : 0.016007818281650543
Loss at batch 190 : 0.011893532238900661
Loss at batch 200 : 0.0194773580878973
Loss at batch 210 : 0.011496186256408691
Loss at batch 220 : 0.009784872643649578
Loss at batch 230 : 0.011314298957586288
Loss at batch 240 : 0.031491052359342575
Loss at batch 250 : 0.014131002128124237
Loss at batch 260 : 0.01600603386759758
Loss at batch 270 : 0.02795860543847084
Loss at batch 280 : 0.010492184199392796
Loss at batch 290 : 0.012410806491971016
Loss at batch 300 : 0.018923267722129822
Loss at batch 310 : 0.008565502241253853
Loss at batch 320 : 0.010602055117487907
Loss at batch 330 : 0.013422190211713314
Loss at batch 340 : 0.010246268473565578
Loss at batch 350 : 0.02007417567074299
Loss at batch 360 : 0.02423514612019062
Loss at batch 370 : 0.011358658783137798
epoch59 finished!
Loss at batch 10 : 0.011321218684315681
Loss at batch 20 : 0.01432747207581997
Loss at batch 30 : 0.013228774070739746
Loss at batch 40 : 0.015970658510923386
Loss at batch 50 : 0.012917453423142433
Loss at batch 60 : 0.019066594541072845
Loss at batch 70 : 0.0155473118647933
Loss at batch 80 : 0.014477350749075413
Loss at batch 90 : 0.009632742032408714
Loss at batch 100 : 0.025340856984257698
Loss at batch 110 : 0.015212141908705235
Loss at batch 120 : 0.011660727672278881
Loss at batch 130 : 0.011761442758142948
Loss at batch 140 : 0.008811520412564278
Loss at batch 150 : 0.01336838211864233
Loss at batch 160 : 0.019032558426260948
Loss at batch 170 : 0.022626562044024467
Loss at batch 180 : 0.01837797276675701
Loss at batch 190 : 0.010563808493316174
Loss at batch 200 : 0.014279891736805439
Loss at batch 210 : 0.02705865353345871
Loss at batch 220 : 0.008415774442255497
Loss at batch 230 : 0.01326956320554018
Loss at batch 240 : 0.010195664130151272
Loss at batch 250 : 0.012418391183018684
Loss at batch 260 : 0.010712357237935066
Loss at batch 270 : 0.021178197115659714
Loss at batch 280 : 0.014483917504549026
Loss at batch 290 : 0.009929629042744637
Loss at batch 300 : 0.014524394646286964
Loss at batch 310 : 0.020658675581216812
Loss at batch 320 : 0.010616103187203407
Loss at batch 330 : 0.012488440610468388
Loss at batch 340 : 0.012802353128790855
Loss at batch 350 : 0.012497895397245884
Loss at batch 360 : 0.01399815734475851
Loss at batch 370 : 0.02046487294137478
epoch60 finished!
Loss at batch 10 : 0.021499957889318466
Loss at batch 20 : 0.009233666583895683
Loss at batch 30 : 0.01398332417011261
Loss at batch 40 : 0.009166819043457508
Loss at batch 50 : 0.013583874329924583
Loss at batch 60 : 0.017447801306843758
Loss at batch 70 : 0.025656448677182198
Loss at batch 80 : 0.009617314673960209
Loss at batch 90 : 0.012497239746153355
Loss at batch 100 : 0.0363752618432045
Loss at batch 110 : 0.011119400151073933
Loss at batch 120 : 0.023693449795246124
Loss at batch 130 : 0.020778488367795944
Loss at batch 140 : 0.016434844583272934
Loss at batch 150 : 0.016743682324886322
Loss at batch 160 : 0.009890777058899403
Loss at batch 170 : 0.022354481741786003
Loss at batch 180 : 0.010323522612452507
Loss at batch 190 : 0.014757184311747551
Loss at batch 200 : 0.01087917760014534
Loss at batch 210 : 0.018584365025162697
Loss at batch 220 : 0.01008403766900301
Loss at batch 230 : 0.011671890504658222
Loss at batch 240 : 0.01829717680811882
Loss at batch 250 : 0.013234488666057587
Loss at batch 260 : 0.01074509508907795
Loss at batch 270 : 0.014199329540133476
Loss at batch 280 : 0.011192165315151215
Loss at batch 290 : 0.009227336384356022
Loss at batch 300 : 0.012044727802276611
Loss at batch 310 : 0.011984208598732948
Loss at batch 320 : 0.01335847470909357
Loss at batch 330 : 0.008000017143785954
Loss at batch 340 : 0.014928256161510944
Loss at batch 350 : 0.018389910459518433
Loss at batch 360 : 0.012227825820446014
Loss at batch 370 : 0.007593422196805477
epoch61 finished!
Loss at batch 10 : 0.006773900240659714
Loss at batch 20 : 0.012457523494958878
Loss at batch 30 : 0.021323705092072487
Loss at batch 40 : 0.013644208200275898
Loss at batch 50 : 0.015933483839035034
Loss at batch 60 : 0.01471585314720869
Loss at batch 70 : 0.02257177233695984
Loss at batch 80 : 0.01033984124660492
Loss at batch 90 : 0.017782187089323997
Loss at batch 100 : 0.030161065980792046
Loss at batch 110 : 0.01053896825760603
Loss at batch 120 : 0.014789793640375137
Loss at batch 130 : 0.017850525677204132
Loss at batch 140 : 0.006549278739839792
Loss at batch 150 : 0.03013545274734497
Loss at batch 160 : 0.012337960302829742
Loss at batch 170 : 0.009291356429457664
Loss at batch 180 : 0.023151101544499397
Loss at batch 190 : 0.012879298999905586
Loss at batch 200 : 0.014699126593768597
Loss at batch 210 : 0.01247054897248745
Loss at batch 220 : 0.012851794250309467
Loss at batch 230 : 0.016967877745628357
Loss at batch 240 : 0.01663711667060852
Loss at batch 250 : 0.010233164764940739
Loss at batch 260 : 0.01650293357670307
Loss at batch 270 : 0.012002489529550076
Loss at batch 280 : 0.009318782947957516
Loss at batch 290 : 0.00831613875925541
Loss at batch 300 : 0.009268262423574924
Loss at batch 310 : 0.014666530303657055
Loss at batch 320 : 0.013189834542572498
Loss at batch 330 : 0.008488962426781654
Loss at batch 340 : 0.02222326211631298
Loss at batch 350 : 0.019302206113934517
Loss at batch 360 : 0.01772761344909668
Loss at batch 370 : 0.01562637649476528
epoch62 finished!
Loss at batch 10 : 0.020261166617274284
Loss at batch 20 : 0.01742277853190899
Loss at batch 30 : 0.011114424094557762
Loss at batch 40 : 0.017782853916287422
Loss at batch 50 : 0.012881925329566002
Loss at batch 60 : 0.012244491837918758
Loss at batch 70 : 0.010242023505270481
Loss at batch 80 : 0.01532302051782608
Loss at batch 90 : 0.01965682953596115
Loss at batch 100 : 0.023126794025301933
Loss at batch 110 : 0.01860758848488331
Loss at batch 120 : 0.017228417098522186
Loss at batch 130 : 0.012301807291805744
Loss at batch 140 : 0.015887601301074028
Loss at batch 150 : 0.010228065773844719
Loss at batch 160 : 0.013266569003462791
Loss at batch 170 : 0.0091587258502841
Loss at batch 180 : 0.022025272250175476
Loss at batch 190 : 0.012524386867880821
Loss at batch 200 : 0.008993436582386494
Loss at batch 210 : 0.01180254016071558
Loss at batch 220 : 0.022537948563694954
Loss at batch 230 : 0.014157861471176147
Loss at batch 240 : 0.012067277915775776
Loss at batch 250 : 0.015239860862493515
Loss at batch 260 : 0.010784098878502846
Loss at batch 270 : 0.006577705964446068
Loss at batch 280 : 0.010910083539783955
Loss at batch 290 : 0.017342953011393547
Loss at batch 300 : 0.014190887100994587
Loss at batch 310 : 0.010647865943610668
Loss at batch 320 : 0.014569155871868134
Loss at batch 330 : 0.016040287911891937
Loss at batch 340 : 0.014374583959579468
Loss at batch 350 : 0.015951544046401978
Loss at batch 360 : 0.008999267593026161
Loss at batch 370 : 0.016815748065710068
epoch63 finished!
Loss at batch 10 : 0.011830148287117481
Loss at batch 20 : 0.01736447773873806
Loss at batch 30 : 0.025073552504181862
Loss at batch 40 : 0.014409234747290611
Loss at batch 50 : 0.013515828177332878
Loss at batch 60 : 0.01560219842940569
Loss at batch 70 : 0.01198923122137785
Loss at batch 80 : 0.02325602062046528
Loss at batch 90 : 0.013582310639321804
Loss at batch 100 : 0.023003673180937767
Loss at batch 110 : 0.01889737695455551
Loss at batch 120 : 0.009785937145352364
Loss at batch 130 : 0.018156807869672775
Loss at batch 140 : 0.02309918776154518
Loss at batch 150 : 0.012961059808731079
Loss at batch 160 : 0.014943396672606468
Loss at batch 170 : 0.01436979603022337
Loss at batch 180 : 0.008160308003425598
Loss at batch 190 : 0.013314859010279179
Loss at batch 200 : 0.018694115802645683
Loss at batch 210 : 0.010832025669515133
Loss at batch 220 : 0.015821445733308792
Loss at batch 230 : 0.014072144404053688
Loss at batch 240 : 0.00784182921051979
Loss at batch 250 : 0.037294935435056686
Loss at batch 260 : 0.011763975955545902
Loss at batch 270 : 0.020005064085125923
Loss at batch 280 : 0.015978604555130005
Loss at batch 290 : 0.012694564647972584
Loss at batch 300 : 0.01608940400183201
Loss at batch 310 : 0.010582301765680313
Loss at batch 320 : 0.010150274261832237
Loss at batch 330 : 0.008729483932256699
Loss at batch 340 : 0.011921235360205173
Loss at batch 350 : 0.026569142937660217
Loss at batch 360 : 0.011159390211105347
Loss at batch 370 : 0.01781618781387806
epoch64 finished!
Loss at batch 10 : 0.015136475674808025
Loss at batch 20 : 0.015116401948034763
Loss at batch 30 : 0.02314172498881817
Loss at batch 40 : 0.01733563281595707
Loss at batch 50 : 0.013637423515319824
Loss at batch 60 : 0.015849070623517036
Loss at batch 70 : 0.009932776913046837
Loss at batch 80 : 0.014344531111419201
Loss at batch 90 : 0.014485493302345276
Loss at batch 100 : 0.013488862663507462
Loss at batch 110 : 0.014252522960305214
Loss at batch 120 : 0.021522270515561104
Loss at batch 130 : 0.011173422448337078
Loss at batch 140 : 0.014354574494063854
Loss at batch 150 : 0.01714400202035904
Loss at batch 160 : 0.005610326305031776
Loss at batch 170 : 0.029671357944607735
Loss at batch 180 : 0.013013261370360851
Loss at batch 190 : 0.014489689841866493
Loss at batch 200 : 0.017284709960222244
Loss at batch 210 : 0.014229843392968178
Loss at batch 220 : 0.012437068857252598
Loss at batch 230 : 0.010349386371672153
Loss at batch 240 : 0.009076463989913464
Loss at batch 250 : 0.024340061470866203
Loss at batch 260 : 0.026920760050415993
Loss at batch 270 : 0.00973097700625658
Loss at batch 280 : 0.028633106499910355
Loss at batch 290 : 0.018823420628905296
Loss at batch 300 : 0.020712869241833687
Loss at batch 310 : 0.00835034903138876
Loss at batch 320 : 0.016653824597597122
Loss at batch 330 : 0.012876633554697037
Loss at batch 340 : 0.012551354244351387
Loss at batch 350 : 0.010153927840292454
Loss at batch 360 : 0.014078388921916485
Loss at batch 370 : 0.02008512243628502
epoch65 finished!
Loss at batch 10 : 0.014254553243517876
Loss at batch 20 : 0.008979640901088715
Loss at batch 30 : 0.009772809222340584
Loss at batch 40 : 0.00985562801361084
Loss at batch 50 : 0.011203712783753872
Loss at batch 60 : 0.013099830597639084
Loss at batch 70 : 0.018065782263875008
Loss at batch 80 : 0.011659717187285423
Loss at batch 90 : 0.022509494796395302
Loss at batch 100 : 0.011169037781655788
Loss at batch 110 : 0.01957765780389309
Loss at batch 120 : 0.006646853871643543
Loss at batch 130 : 0.020680274814367294
Loss at batch 140 : 0.01824330724775791
Loss at batch 150 : 0.014554708264768124
Loss at batch 160 : 0.014901763759553432
Loss at batch 170 : 0.013822153210639954
Loss at batch 180 : 0.012256687507033348
Loss at batch 190 : 0.008617117069661617
Loss at batch 200 : 0.019197504967451096
Loss at batch 210 : 0.01489351224154234
Loss at batch 220 : 0.01639222912490368
Loss at batch 230 : 0.007311976980417967
Loss at batch 240 : 0.011513962410390377
Loss at batch 250 : 0.013659998774528503
Loss at batch 260 : 0.009790511801838875
Loss at batch 270 : 0.024118654429912567
Loss at batch 280 : 0.012561256997287273
Loss at batch 290 : 0.01718907058238983
Loss at batch 300 : 0.015271224081516266
Loss at batch 310 : 0.01836250349879265
Loss at batch 320 : 0.01527733076363802
Loss at batch 330 : 0.033543311059474945
Loss at batch 340 : 0.024714650586247444
Loss at batch 350 : 0.010647610761225224
Loss at batch 360 : 0.015701865777373314
Loss at batch 370 : 0.014975289814174175
epoch66 finished!
Loss at batch 10 : 0.012717080302536488
Loss at batch 20 : 0.01575407199561596
Loss at batch 30 : 0.01487858034670353
Loss at batch 40 : 0.013718944042921066
Loss at batch 50 : 0.008571038022637367
Loss at batch 60 : 0.015793340280652046
Loss at batch 70 : 0.019282614812254906
Loss at batch 80 : 0.018365444615483284
Loss at batch 90 : 0.013574235141277313
Loss at batch 100 : 0.01652446761727333
Loss at batch 110 : 0.019087431952357292
Loss at batch 120 : 0.011923711746931076
Loss at batch 130 : 0.008862701244652271
Loss at batch 140 : 0.013923673890531063
Loss at batch 150 : 0.023022694513201714
Loss at batch 160 : 0.012738612480461597
Loss at batch 170 : 0.008911725133657455
Loss at batch 180 : 0.015139644034206867
Loss at batch 190 : 0.012626317329704762
Loss at batch 200 : 0.010365281254053116
Loss at batch 210 : 0.012980664148926735
Loss at batch 220 : 0.008654388599097729
Loss at batch 230 : 0.020163537934422493
Loss at batch 240 : 0.024082869291305542
Loss at batch 250 : 0.014842010103166103
Loss at batch 260 : 0.01203425507992506
Loss at batch 270 : 0.008377501741051674
Loss at batch 280 : 0.010902886278927326
Loss at batch 290 : 0.007739826571196318
Loss at batch 300 : 0.019512761384248734
Loss at batch 310 : 0.015116467140614986
Loss at batch 320 : 0.009257389232516289
Loss at batch 330 : 0.01811843551695347
Loss at batch 340 : 0.00793789979070425
Loss at batch 350 : 0.016713153570890427
Loss at batch 360 : 0.01915748231112957
Loss at batch 370 : 0.0190588366240263
epoch67 finished!
Loss at batch 10 : 0.02436530962586403
Loss at batch 20 : 0.02349557727575302
Loss at batch 30 : 0.022403031587600708
Loss at batch 40 : 0.019577903673052788
Loss at batch 50 : 0.012411760166287422
Loss at batch 60 : 0.025661863386631012
Loss at batch 70 : 0.02309112623333931
Loss at batch 80 : 0.013539697974920273
Loss at batch 90 : 0.008670330978929996
Loss at batch 100 : 0.02120053581893444
Loss at batch 110 : 0.008732262998819351
Loss at batch 120 : 0.02908976748585701
Loss at batch 130 : 0.006417742930352688
Loss at batch 140 : 0.017681444063782692
Loss at batch 150 : 0.013730582781136036
Loss at batch 160 : 0.016943102702498436
Loss at batch 170 : 0.01579553820192814
Loss at batch 180 : 0.012151572853326797
Loss at batch 190 : 0.014475097879767418
Loss at batch 200 : 0.01793290488421917
Loss at batch 210 : 0.008888364769518375
Loss at batch 220 : 0.040921859443187714
Loss at batch 230 : 0.01750960759818554
Loss at batch 240 : 0.02581421472132206
Loss at batch 250 : 0.016233431175351143
Loss at batch 260 : 0.009551809169352055
Loss at batch 270 : 0.008665024302899837
Loss at batch 280 : 0.012169577181339264
Loss at batch 290 : 0.014072894118726254
Loss at batch 300 : 0.01126621663570404
Loss at batch 310 : 0.01789969950914383
Loss at batch 320 : 0.01979346200823784
Loss at batch 330 : 0.009588598273694515
Loss at batch 340 : 0.017895756289362907
Loss at batch 350 : 0.03001118078827858
Loss at batch 360 : 0.02192055433988571
Loss at batch 370 : 0.010546984151005745
epoch68 finished!
Loss at batch 10 : 0.020669883117079735
Loss at batch 20 : 0.01233681570738554
Loss at batch 30 : 0.011519345454871655
Loss at batch 40 : 0.023833731189370155
Loss at batch 50 : 0.031868692487478256
Loss at batch 60 : 0.0087543074041605
Loss at batch 70 : 0.0176052488386631
Loss at batch 80 : 0.016570892184972763
Loss at batch 90 : 0.007832251489162445
Loss at batch 100 : 0.01956818997859955
Loss at batch 110 : 0.010584456846117973
Loss at batch 120 : 0.020990243181586266
Loss at batch 130 : 0.011635279282927513
Loss at batch 140 : 0.020524200052022934
Loss at batch 150 : 0.012094924226403236
Loss at batch 160 : 0.012216366827487946
Loss at batch 170 : 0.026177385821938515
Loss at batch 180 : 0.012655364349484444
Loss at batch 190 : 0.02201708033680916
Loss at batch 200 : 0.02284814417362213
Loss at batch 210 : 0.008952630683779716
Loss at batch 220 : 0.019247159361839294
Loss at batch 230 : 0.006857437081634998
Loss at batch 240 : 0.01411216426640749
Loss at batch 250 : 0.011052409186959267
Loss at batch 260 : 0.014408327639102936
Loss at batch 270 : 0.015565088018774986
Loss at batch 280 : 0.02311217039823532
Loss at batch 290 : 0.011496366001665592
Loss at batch 300 : 0.0187528058886528
Loss at batch 310 : 0.0068969144485890865
Loss at batch 320 : 0.013822467997670174
Loss at batch 330 : 0.013948192819952965
Loss at batch 340 : 0.013049113564193249
Loss at batch 350 : 0.011554824188351631
Loss at batch 360 : 0.011919539421796799
Loss at batch 370 : 0.01125168614089489
epoch69 finished!
Loss at batch 10 : 0.0155706238001585
Loss at batch 20 : 0.008444521576166153
Loss at batch 30 : 0.026410996913909912
Loss at batch 40 : 0.010663346387445927
Loss at batch 50 : 0.013895976357161999
Loss at batch 60 : 0.018622901290655136
Loss at batch 70 : 0.011415519751608372
Loss at batch 80 : 0.008888506330549717
Loss at batch 90 : 0.02032836526632309
Loss at batch 100 : 0.013661961071193218
Loss at batch 110 : 0.030199937522411346
Loss at batch 120 : 0.014236344955861568
Loss at batch 130 : 0.009099122136831284
Loss at batch 140 : 0.018426336348056793
Loss at batch 150 : 0.02446899749338627
Loss at batch 160 : 0.010761049576103687
Loss at batch 170 : 0.009380669333040714
Loss at batch 180 : 0.009257304482161999
Loss at batch 190 : 0.016606662422418594
Loss at batch 200 : 0.018704580143094063
Loss at batch 210 : 0.021436776965856552
Loss at batch 220 : 0.014146623201668262
Loss at batch 230 : 0.019824199378490448
Loss at batch 240 : 0.010806407779455185
Loss at batch 250 : 0.018226230517029762
Loss at batch 260 : 0.021601365879178047
Loss at batch 270 : 0.015598131343722343
Loss at batch 280 : 0.010220917873084545
Loss at batch 290 : 0.012239687144756317
Loss at batch 300 : 0.015957364812493324
Loss at batch 310 : 0.021754417568445206
Loss at batch 320 : 0.014709608629345894
Loss at batch 330 : 0.007175623904913664
Loss at batch 340 : 0.017503418028354645
Loss at batch 350 : 0.010119711980223656
Loss at batch 360 : 0.018531236797571182
Loss at batch 370 : 0.007710400503128767
epoch70 finished!
Loss at batch 10 : 0.02045852690935135
Loss at batch 20 : 0.015404989942908287
Loss at batch 30 : 0.010173224844038486
Loss at batch 40 : 0.031209582462906837
Loss at batch 50 : 0.011812136508524418
Loss at batch 60 : 0.010056733153760433
Loss at batch 70 : 0.011546287685632706
Loss at batch 80 : 0.012749405577778816
Loss at batch 90 : 0.011188948526978493
Loss at batch 100 : 0.03269340470433235
Loss at batch 110 : 0.011207593604922295
Loss at batch 120 : 0.012677635997533798
Loss at batch 130 : 0.025499433279037476
Loss at batch 140 : 0.011088690720498562
Loss at batch 150 : 0.018959201872348785
Loss at batch 160 : 0.005814287345856428
Loss at batch 170 : 0.009974125772714615
Loss at batch 180 : 0.01600789651274681
Loss at batch 190 : 0.016605162993073463
Loss at batch 200 : 0.011357784271240234
Loss at batch 210 : 0.01232215017080307
Loss at batch 220 : 0.016177834942936897
Loss at batch 230 : 0.017993533983826637
Loss at batch 240 : 0.016203688457608223
Loss at batch 250 : 0.018012113869190216
Loss at batch 260 : 0.009735414758324623
Loss at batch 270 : 0.011821146123111248
Loss at batch 280 : 0.016373280435800552
Loss at batch 290 : 0.008614351972937584
Loss at batch 300 : 0.019618965685367584
Loss at batch 310 : 0.018149124458432198
Loss at batch 320 : 0.017843015491962433
Loss at batch 330 : 0.01537996344268322
Loss at batch 340 : 0.01154770515859127
Loss at batch 350 : 0.01113393995910883
Loss at batch 360 : 0.01199688483029604
Loss at batch 370 : 0.01844796910881996
epoch71 finished!
Loss at batch 10 : 0.023722363635897636
Loss at batch 20 : 0.010560061782598495
Loss at batch 30 : 0.013805869035422802
Loss at batch 40 : 0.01058595534414053
Loss at batch 50 : 0.02761850133538246
Loss at batch 60 : 0.011656036600470543
Loss at batch 70 : 0.025831082835793495
Loss at batch 80 : 0.011644836515188217
Loss at batch 90 : 0.014331601560115814
Loss at batch 100 : 0.011083956807851791
Loss at batch 110 : 0.017401976510882378
Loss at batch 120 : 0.02106061950325966
Loss at batch 130 : 0.017695659771561623
Loss at batch 140 : 0.009755362756550312
Loss at batch 150 : 0.012893298640847206
Loss at batch 160 : 0.018977157771587372
Loss at batch 170 : 0.021585265174508095
Loss at batch 180 : 0.01633986085653305
Loss at batch 190 : 0.02532946690917015
Loss at batch 200 : 0.010865547694265842
Loss at batch 210 : 0.011695381253957748
Loss at batch 220 : 0.006611175835132599
Loss at batch 230 : 0.011328485794365406
Loss at batch 240 : 0.015818627551198006
Loss at batch 250 : 0.011796535924077034
Loss at batch 260 : 0.017913488671183586
Loss at batch 270 : 0.013664845377206802
Loss at batch 280 : 0.01948239840567112
Loss at batch 290 : 0.013937525451183319
Loss at batch 300 : 0.010297233238816261
Loss at batch 310 : 0.01984982006251812
Loss at batch 320 : 0.012931521050632
Loss at batch 330 : 0.007445471361279488
Loss at batch 340 : 0.006808700505644083
Loss at batch 350 : 0.017415178939700127
Loss at batch 360 : 0.037608321756124496
Loss at batch 370 : 0.008312183432281017
epoch72 finished!
Loss at batch 10 : 0.021964946761727333
Loss at batch 20 : 0.011947148479521275
Loss at batch 30 : 0.0167132206261158
Loss at batch 40 : 0.016884008422493935
Loss at batch 50 : 0.01678469218313694
Loss at batch 60 : 0.014421628788113594
Loss at batch 70 : 0.016169164329767227
Loss at batch 80 : 0.010001298040151596
Loss at batch 90 : 0.012247039936482906
Loss at batch 100 : 0.01741088181734085
Loss at batch 110 : 0.012747352011501789
Loss at batch 120 : 0.015122477896511555
Loss at batch 130 : 0.02195177786052227
Loss at batch 140 : 0.02513216994702816
Loss at batch 150 : 0.008103955537080765
Loss at batch 160 : 0.011636069975793362
Loss at batch 170 : 0.01585901342332363
Loss at batch 180 : 0.01336599700152874
Loss at batch 190 : 0.013732164166867733
Loss at batch 200 : 0.01266427244991064
Loss at batch 210 : 0.01107643824070692
Loss at batch 220 : 0.019746871665120125
Loss at batch 230 : 0.015346202068030834
Loss at batch 240 : 0.009716159664094448
Loss at batch 250 : 0.009069827385246754
Loss at batch 260 : 0.009516632184386253
Loss at batch 270 : 0.01604919135570526
Loss at batch 280 : 0.012788007967174053
Loss at batch 290 : 0.026635747402906418
Loss at batch 300 : 0.0176011323928833
Loss at batch 310 : 0.012042233720421791
Loss at batch 320 : 0.017746709287166595
Loss at batch 330 : 0.020282406359910965
Loss at batch 340 : 0.01709083840250969
Loss at batch 350 : 0.02697007544338703
Loss at batch 360 : 0.020362848415970802
Loss at batch 370 : 0.018635371699929237
epoch73 finished!
Loss at batch 10 : 0.020574945956468582
Loss at batch 20 : 0.02223723940551281
Loss at batch 30 : 0.013906829059123993
Loss at batch 40 : 0.014501365832984447
Loss at batch 50 : 0.013560613617300987
Loss at batch 60 : 0.010719976387917995
Loss at batch 70 : 0.02068954147398472
Loss at batch 80 : 0.007810504175722599
Loss at batch 90 : 0.019756432622671127
Loss at batch 100 : 0.0173358004540205
Loss at batch 110 : 0.01676219142973423
Loss at batch 120 : 0.009710528887808323
Loss at batch 130 : 0.008865460753440857
Loss at batch 140 : 0.012264392338693142
Loss at batch 150 : 0.02183893509209156
Loss at batch 160 : 0.01204449962824583
Loss at batch 170 : 0.011290840804576874
Loss at batch 180 : 0.026685219258069992
Loss at batch 190 : 0.01117614097893238
Loss at batch 200 : 0.01108736265450716
Loss at batch 210 : 0.03126051276922226
Loss at batch 220 : 0.010206174105405807
Loss at batch 230 : 0.009022839367389679
Loss at batch 240 : 0.017594851553440094
Loss at batch 250 : 0.012562304735183716
Loss at batch 260 : 0.012933767400681973
Loss at batch 270 : 0.026446063071489334
Loss at batch 280 : 0.008223142474889755
Loss at batch 290 : 0.017684681341052055
Loss at batch 300 : 0.014374404214322567
Loss at batch 310 : 0.015549581497907639
Loss at batch 320 : 0.012193245813250542
Loss at batch 330 : 0.009584607556462288
Loss at batch 340 : 0.0188923217356205
Loss at batch 350 : 0.007192767690867186
Loss at batch 360 : 0.019381355494260788
Loss at batch 370 : 0.015514891594648361
epoch74 finished!
Loss at batch 10 : 0.019543718546628952
Loss at batch 20 : 0.009107217192649841
Loss at batch 30 : 0.009258823469281197
Loss at batch 40 : 0.012106469832360744
Loss at batch 50 : 0.028771935030817986
Loss at batch 60 : 0.011891257017850876
Loss at batch 70 : 0.013002307154238224
Loss at batch 80 : 0.019057244062423706
Loss at batch 90 : 0.01734728179872036
Loss at batch 100 : 0.012262450531125069
Loss at batch 110 : 0.013950061053037643
Loss at batch 120 : 0.014969986863434315
Loss at batch 130 : 0.009101426228880882
Loss at batch 140 : 0.01570167765021324
Loss at batch 150 : 0.012448756955564022
Loss at batch 160 : 0.01279402058571577
Loss at batch 170 : 0.00906388834118843
Loss at batch 180 : 0.022770728915929794
Loss at batch 190 : 0.009019879624247551
Loss at batch 200 : 0.006767671555280685
Loss at batch 210 : 0.016862230375409126
Loss at batch 220 : 0.01156669296324253
Loss at batch 230 : 0.009469210170209408
Loss at batch 240 : 0.010424765758216381
Loss at batch 250 : 0.011258499696850777
Loss at batch 260 : 0.012355231679975986
Loss at batch 270 : 0.03132639825344086
Loss at batch 280 : 0.017735250294208527
Loss at batch 290 : 0.011925947852432728
Loss at batch 300 : 0.011483841575682163
Loss at batch 310 : 0.021611735224723816
Loss at batch 320 : 0.008053066208958626
Loss at batch 330 : 0.009634838439524174
Loss at batch 340 : 0.018299316987395287
Loss at batch 350 : 0.0086217587813735
Loss at batch 360 : 0.010434231720864773
Loss at batch 370 : 0.0102548748254776
epoch75 finished!
Loss at batch 10 : 0.020645227283239365
Loss at batch 20 : 0.02698996104300022
Loss at batch 30 : 0.020133057609200478
Loss at batch 40 : 0.015304121188819408
Loss at batch 50 : 0.01688932441174984
Loss at batch 60 : 0.008729656226933002
Loss at batch 70 : 0.016200672835111618
Loss at batch 80 : 0.00697889132425189
Loss at batch 90 : 0.008924007415771484
Loss at batch 100 : 0.007967939600348473
Loss at batch 110 : 0.02215154841542244
Loss at batch 120 : 0.011684305034577847
Loss at batch 130 : 0.012061101384460926
Loss at batch 140 : 0.01395660825073719
Loss at batch 150 : 0.007959210313856602
Loss at batch 160 : 0.01403588056564331
Loss at batch 170 : 0.020043134689331055
Loss at batch 180 : 0.025232959538698196
Loss at batch 190 : 0.012223536148667336
Loss at batch 200 : 0.02346479706466198
Loss at batch 210 : 0.016654515638947487
Loss at batch 220 : 0.01521454006433487
Loss at batch 230 : 0.0114898681640625
Loss at batch 240 : 0.014883194118738174
Loss at batch 250 : 0.014003599062561989
Loss at batch 260 : 0.02026480995118618
Loss at batch 270 : 0.017679287120699883
Loss at batch 280 : 0.010912793688476086
Loss at batch 290 : 0.018583234399557114
Loss at batch 300 : 0.0332350954413414
Loss at batch 310 : 0.0205130185931921
Loss at batch 320 : 0.01094910316169262
Loss at batch 330 : 0.011027880944311619
Loss at batch 340 : 0.008016710169613361
Loss at batch 350 : 0.014474211260676384
Loss at batch 360 : 0.012817736715078354
Loss at batch 370 : 0.024254150688648224
epoch76 finished!
Loss at batch 10 : 0.011137334629893303
Loss at batch 20 : 0.012415616773068905
Loss at batch 30 : 0.019287822768092155
Loss at batch 40 : 0.012219151481986046
Loss at batch 50 : 0.019111262634396553
Loss at batch 60 : 0.00626192195340991
Loss at batch 70 : 0.02400362864136696
Loss at batch 80 : 0.019811993464827538
Loss at batch 90 : 0.00828530453145504
Loss at batch 100 : 0.01478529628366232
Loss at batch 110 : 0.01909700222313404
Loss at batch 120 : 0.014678147621452808
Loss at batch 130 : 0.009022017940878868
Loss at batch 140 : 0.007762357126921415
Loss at batch 150 : 0.012869748286902905
Loss at batch 160 : 0.021215137094259262
Loss at batch 170 : 0.008716315031051636
Loss at batch 180 : 0.01116955652832985
Loss at batch 190 : 0.010029088705778122
Loss at batch 200 : 0.018108194693922997
Loss at batch 210 : 0.01329355500638485
Loss at batch 220 : 0.012144781649112701
Loss at batch 230 : 0.007974092848598957
Loss at batch 240 : 0.010119171813130379
Loss at batch 250 : 0.007762633264064789
Loss at batch 260 : 0.01648293063044548
Loss at batch 270 : 0.015469127334654331
Loss at batch 280 : 0.01322944462299347
Loss at batch 290 : 0.012829740531742573
Loss at batch 300 : 0.018883926793932915
Loss at batch 310 : 0.016568567603826523
Loss at batch 320 : 0.015033622272312641
Loss at batch 330 : 0.013012656010687351
Loss at batch 340 : 0.010472489520907402
Loss at batch 350 : 0.016022318974137306
Loss at batch 360 : 0.023647330701351166
Loss at batch 370 : 0.029485372826457024
epoch77 finished!
Loss at batch 10 : 0.016109900549054146
Loss at batch 20 : 0.008981102146208286
Loss at batch 30 : 0.01842062920331955
Loss at batch 40 : 0.009801413863897324
Loss at batch 50 : 0.016735291108489037
Loss at batch 60 : 0.012050573714077473
Loss at batch 70 : 0.007905149832367897
Loss at batch 80 : 0.01602005399763584
Loss at batch 90 : 0.008256207220256329
Loss at batch 100 : 0.012788018211722374
Loss at batch 110 : 0.01754988729953766
Loss at batch 120 : 0.014010421000421047
Loss at batch 130 : 0.026195716112852097
Loss at batch 140 : 0.006401080172508955
Loss at batch 150 : 0.011029325425624847
Loss at batch 160 : 0.00916286464780569
Loss at batch 170 : 0.014919327571988106
Loss at batch 180 : 0.01165201049298048
Loss at batch 190 : 0.010724901221692562
Loss at batch 200 : 0.019292442128062248
Loss at batch 210 : 0.011652898043394089
Loss at batch 220 : 0.008281692862510681
Loss at batch 230 : 0.008860079571604729
Loss at batch 240 : 0.019000913947820663
Loss at batch 250 : 0.01800789311528206
Loss at batch 260 : 0.018009327352046967
Loss at batch 270 : 0.012178457342088223
Loss at batch 280 : 0.01122358813881874
Loss at batch 290 : 0.014971469528973103
Loss at batch 300 : 0.009414169937372208
Loss at batch 310 : 0.011912518180906773
Loss at batch 320 : 0.013459441252052784
Loss at batch 330 : 0.02057892456650734
Loss at batch 340 : 0.009686991572380066
Loss at batch 350 : 0.014357172884047031
Loss at batch 360 : 0.017833128571510315
Loss at batch 370 : 0.007275321986526251
epoch78 finished!
Loss at batch 10 : 0.008405554108321667
Loss at batch 20 : 0.018702294677495956
Loss at batch 30 : 0.010578653775155544
Loss at batch 40 : 0.01919896900653839
Loss at batch 50 : 0.008258734829723835
Loss at batch 60 : 0.01583344116806984
Loss at batch 70 : 0.01817013882100582
Loss at batch 80 : 0.011542515829205513
Loss at batch 90 : 0.02616654336452484
Loss at batch 100 : 0.01909581944346428
Loss at batch 110 : 0.009778151288628578
Loss at batch 120 : 0.014055879786610603
Loss at batch 130 : 0.006945999804884195
Loss at batch 140 : 0.01732211746275425
Loss at batch 150 : 0.012271442450582981
Loss at batch 160 : 0.025310270488262177
Loss at batch 170 : 0.017383234575390816
Loss at batch 180 : 0.014120791107416153
Loss at batch 190 : 0.012508740648627281
Loss at batch 200 : 0.015707148239016533
Loss at batch 210 : 0.014882907271385193
Loss at batch 220 : 0.009380859322845936
Loss at batch 230 : 0.007802707143127918
Loss at batch 240 : 0.011421795934438705
Loss at batch 250 : 0.03466307744383812
Loss at batch 260 : 0.014858902432024479
Loss at batch 270 : 0.011559579521417618
Loss at batch 280 : 0.009914295747876167
Loss at batch 290 : 0.013523558154702187
Loss at batch 300 : 0.0185893215239048
Loss at batch 310 : 0.014857972972095013
Loss at batch 320 : 0.014675489626824856
Loss at batch 330 : 0.014680224470794201
Loss at batch 340 : 0.017605900764465332
Loss at batch 350 : 0.022029371932148933
Loss at batch 360 : 0.011601729318499565
Loss at batch 370 : 0.0245094895362854
epoch79 finished!
Loss at batch 10 : 0.006960064638406038
Loss at batch 20 : 0.011277289129793644
Loss at batch 30 : 0.013196581043303013
Loss at batch 40 : 0.013911488465964794
Loss at batch 50 : 0.012661390006542206
Loss at batch 60 : 0.008688305504620075
Loss at batch 70 : 0.018897641450166702
Loss at batch 80 : 0.015334147028625011
Loss at batch 90 : 0.013638189062476158
Loss at batch 100 : 0.018901748582720757
Loss at batch 110 : 0.020489515736699104
Loss at batch 120 : 0.00937114842236042
Loss at batch 130 : 0.012547018937766552
Loss at batch 140 : 0.017754852771759033
Loss at batch 150 : 0.01432990562170744
Loss at batch 160 : 0.01818144880235195
Loss at batch 170 : 0.010570301674306393
Loss at batch 180 : 0.015159709379076958
Loss at batch 190 : 0.007336928974837065
Loss at batch 200 : 0.012916557490825653
Loss at batch 210 : 0.009842194616794586
Loss at batch 220 : 0.011132122948765755
Loss at batch 230 : 0.010801670141518116
Loss at batch 240 : 0.011776985600590706
Loss at batch 250 : 0.018144268542528152
Loss at batch 260 : 0.015876835212111473
Loss at batch 270 : 0.012876106426119804
Loss at batch 280 : 0.009542630054056644
Loss at batch 290 : 0.018834015354514122
Loss at batch 300 : 0.009868506342172623
Loss at batch 310 : 0.014974946156144142
Loss at batch 320 : 0.021780654788017273
Loss at batch 330 : 0.012111960910260677
Loss at batch 340 : 0.018301209434866905
Loss at batch 350 : 0.02027863822877407
Loss at batch 360 : 0.014120119623839855
Loss at batch 370 : 0.0160574521869421
epoch80 finished!
Loss at batch 10 : 0.011304217390716076
Loss at batch 20 : 0.018946709111332893
Loss at batch 30 : 0.014145874418318272
Loss at batch 40 : 0.010895517654716969
Loss at batch 50 : 0.020006893202662468
Loss at batch 60 : 0.016889577731490135
Loss at batch 70 : 0.013761026784777641
Loss at batch 80 : 0.010849381797015667
Loss at batch 90 : 0.02028626762330532
Loss at batch 100 : 0.010382251814007759
Loss at batch 110 : 0.00742255337536335
Loss at batch 120 : 0.007919061928987503
Loss at batch 130 : 0.007384229451417923
Loss at batch 140 : 0.01312478817999363
Loss at batch 150 : 0.017858106642961502
Loss at batch 160 : 0.011842813342809677
Loss at batch 170 : 0.019778495654463768
Loss at batch 180 : 0.021743936464190483
Loss at batch 190 : 0.032494377344846725
Loss at batch 200 : 0.016085591167211533
Loss at batch 210 : 0.008435335010290146
Loss at batch 220 : 0.008300446905195713
Loss at batch 230 : 0.014186115935444832
Loss at batch 240 : 0.012620612978935242
Loss at batch 250 : 0.02199542708694935
Loss at batch 260 : 0.008243526332080364
Loss at batch 270 : 0.014284315519034863
Loss at batch 280 : 0.009464605711400509
Loss at batch 290 : 0.014433734118938446
Loss at batch 300 : 0.025753699243068695
Loss at batch 310 : 0.011985893361270428
Loss at batch 320 : 0.010798696428537369
Loss at batch 330 : 0.0083615742623806
Loss at batch 340 : 0.012295437045395374
Loss at batch 350 : 0.013491394929587841
Loss at batch 360 : 0.013550025410950184
Loss at batch 370 : 0.012198138982057571
epoch81 finished!
Loss at batch 10 : 0.017286084592342377
Loss at batch 20 : 0.019721273332834244
Loss at batch 30 : 0.01094493456184864
Loss at batch 40 : 0.014965848065912724
Loss at batch 50 : 0.018103066831827164
Loss at batch 60 : 0.015238359570503235
Loss at batch 70 : 0.02156049944460392
Loss at batch 80 : 0.009849832393229008
Loss at batch 90 : 0.011198927648365498
Loss at batch 100 : 0.012844977900385857
Loss at batch 110 : 0.01089414767920971
Loss at batch 120 : 0.010519715026021004
Loss at batch 130 : 0.020507605746388435
Loss at batch 140 : 0.02560262940824032
Loss at batch 150 : 0.00866249855607748
Loss at batch 160 : 0.0201945248991251
Loss at batch 170 : 0.013076230883598328
Loss at batch 180 : 0.014368557371199131
Loss at batch 190 : 0.016013128682971
Loss at batch 200 : 0.013342115096747875
Loss at batch 210 : 0.017433417961001396
Loss at batch 220 : 0.014180796220898628
Loss at batch 230 : 0.017583368346095085
Loss at batch 240 : 0.007418627385050058
Loss at batch 250 : 0.014653372578322887
Loss at batch 260 : 0.013131890445947647
Loss at batch 270 : 0.010911072604358196
Loss at batch 280 : 0.010365006513893604
Loss at batch 290 : 0.018038809299468994
Loss at batch 300 : 0.015766723081469536
Loss at batch 310 : 0.01368645392358303
Loss at batch 320 : 0.0095784617587924
Loss at batch 330 : 0.011687821708619595
Loss at batch 340 : 0.01320440974086523
Loss at batch 350 : 0.009844288229942322
Loss at batch 360 : 0.013925735838711262
Loss at batch 370 : 0.01718500442802906
epoch82 finished!
Loss at batch 10 : 0.010049130767583847
Loss at batch 20 : 0.01319651398807764
Loss at batch 30 : 0.022705551236867905
Loss at batch 40 : 0.014891949482262135
Loss at batch 50 : 0.01237691380083561
Loss at batch 60 : 0.014320754446089268
Loss at batch 70 : 0.014774295501410961
Loss at batch 80 : 0.016474124044179916
Loss at batch 90 : 0.02931787259876728
Loss at batch 100 : 0.018782658502459526
Loss at batch 110 : 0.01174163818359375
Loss at batch 120 : 0.018459640443325043
Loss at batch 130 : 0.01992456614971161
Loss at batch 140 : 0.010438709519803524
Loss at batch 150 : 0.011112985201179981
Loss at batch 160 : 0.02276761457324028
Loss at batch 170 : 0.012034052051603794
Loss at batch 180 : 0.008659614250063896
Loss at batch 190 : 0.011450341902673244
Loss at batch 200 : 0.027409857138991356
Loss at batch 210 : 0.01346932165324688
Loss at batch 220 : 0.009760779328644276
Loss at batch 230 : 0.012178342789411545
Loss at batch 240 : 0.012881296686828136
Loss at batch 250 : 0.005991136189550161
Loss at batch 260 : 0.005712270736694336
Loss at batch 270 : 0.01685343310236931
Loss at batch 280 : 0.009530991315841675
Loss at batch 290 : 0.01758895255625248
Loss at batch 300 : 0.01431678980588913
Loss at batch 310 : 0.008239527232944965
Loss at batch 320 : 0.01101111713796854
Loss at batch 330 : 0.007389627397060394
Loss at batch 340 : 0.00825255922973156
Loss at batch 350 : 0.009332391433417797
Loss at batch 360 : 0.028405694290995598
Loss at batch 370 : 0.015850575640797615
epoch83 finished!
Loss at batch 10 : 0.018223462626338005
Loss at batch 20 : 0.01650053635239601
Loss at batch 30 : 0.00876429583877325
Loss at batch 40 : 0.022636180743575096
Loss at batch 50 : 0.012339620850980282
Loss at batch 60 : 0.006906596012413502
Loss at batch 70 : 0.010942700318992138
Loss at batch 80 : 0.013405103236436844
Loss at batch 90 : 0.020070958882570267
Loss at batch 100 : 0.018722988665103912
Loss at batch 110 : 0.019302714616060257
Loss at batch 120 : 0.007231538649648428
Loss at batch 130 : 0.014508604072034359
Loss at batch 140 : 0.01905457302927971
Loss at batch 150 : 0.019971352070569992
Loss at batch 160 : 0.015447786077857018
Loss at batch 170 : 0.008554179221391678
Loss at batch 180 : 0.01476257387548685
Loss at batch 190 : 0.009256618097424507
Loss at batch 200 : 0.014605145901441574
Loss at batch 210 : 0.02213435247540474
Loss at batch 220 : 0.010795699432492256
Loss at batch 230 : 0.021826090291142464
Loss at batch 240 : 0.0073111881501972675
Loss at batch 250 : 0.05169941857457161
Loss at batch 260 : 0.012306991033256054
Loss at batch 270 : 0.01600753329694271
Loss at batch 280 : 0.015268230810761452
Loss at batch 290 : 0.012915784493088722
Loss at batch 300 : 0.010756274685263634
Loss at batch 310 : 0.005611872300505638
Loss at batch 320 : 0.017780166119337082
Loss at batch 330 : 0.009765516966581345
Loss at batch 340 : 0.017262782901525497
Loss at batch 350 : 0.006697969511151314
Loss at batch 360 : 0.012776733376085758
Loss at batch 370 : 0.018836675211787224
epoch84 finished!
Loss at batch 10 : 0.009853447787463665
Loss at batch 20 : 0.009685915894806385
Loss at batch 30 : 0.013551352545619011
Loss at batch 40 : 0.030559644103050232
Loss at batch 50 : 0.015705589205026627
Loss at batch 60 : 0.01338751520961523
Loss at batch 70 : 0.008309685625135899
Loss at batch 80 : 0.008521555922925472
Loss at batch 90 : 0.018359839916229248
Loss at batch 100 : 0.012218483723700047
Loss at batch 110 : 0.008310309611260891
Loss at batch 120 : 0.017144177109003067
Loss at batch 130 : 0.014265344478189945
Loss at batch 140 : 0.010961044579744339
Loss at batch 150 : 0.01966239884495735
Loss at batch 160 : 0.009840420447289944
Loss at batch 170 : 0.01726074516773224
Loss at batch 180 : 0.01188693381845951
Loss at batch 190 : 0.010838732123374939
Loss at batch 200 : 0.017330113798379898
Loss at batch 210 : 0.021799596026539803
Loss at batch 220 : 0.012589463032782078
Loss at batch 230 : 0.015594304539263248
Loss at batch 240 : 0.021606534719467163
Loss at batch 250 : 0.008644008077681065
Loss at batch 260 : 0.021550266072154045
Loss at batch 270 : 0.01495643611997366
Loss at batch 280 : 0.010388880968093872
Loss at batch 290 : 0.01391509361565113
Loss at batch 300 : 0.028065834194421768
Loss at batch 310 : 0.009234022349119186
Loss at batch 320 : 0.01109861209988594
Loss at batch 330 : 0.028042608872056007
Loss at batch 340 : 0.027328843250870705
Loss at batch 350 : 0.012461603619158268
Loss at batch 360 : 0.0299996305257082
Loss at batch 370 : 0.011998704634606838
epoch85 finished!
Loss at batch 10 : 0.007574236951768398
Loss at batch 20 : 0.023789502680301666
Loss at batch 30 : 0.014020680449903011
Loss at batch 40 : 0.015413571149110794
Loss at batch 50 : 0.01979142054915428
Loss at batch 60 : 0.014238085597753525
Loss at batch 70 : 0.00676815677434206
Loss at batch 80 : 0.02212175726890564
Loss at batch 90 : 0.009186621755361557
Loss at batch 100 : 0.024977263063192368
Loss at batch 110 : 0.01351296529173851
Loss at batch 120 : 0.020772375166416168
Loss at batch 130 : 0.01654938980937004
Loss at batch 140 : 0.01866910606622696
Loss at batch 150 : 0.01610889844596386
Loss at batch 160 : 0.009333214722573757
Loss at batch 170 : 0.009806628338992596
Loss at batch 180 : 0.024651270359754562
Loss at batch 190 : 0.025706084445118904
Loss at batch 200 : 0.022336311638355255
Loss at batch 210 : 0.018976083025336266
Loss at batch 220 : 0.007204318884760141
Loss at batch 230 : 0.014237499795854092
Loss at batch 240 : 0.009832032956182957
Loss at batch 250 : 0.012378554791212082
Loss at batch 260 : 0.015931671485304832
Loss at batch 270 : 0.01360842864960432
Loss at batch 280 : 0.012353738769888878
Loss at batch 290 : 0.010587855242192745
Loss at batch 300 : 0.01673421636223793
Loss at batch 310 : 0.020334649831056595
Loss at batch 320 : 0.021883578971028328
Loss at batch 330 : 0.020144369453191757
Loss at batch 340 : 0.02364203706383705
Loss at batch 350 : 0.012140251696109772
Loss at batch 360 : 0.009540481492877007
Loss at batch 370 : 0.012672407552599907
epoch86 finished!
Loss at batch 10 : 0.010147584602236748
Loss at batch 20 : 0.010548890568315983
Loss at batch 30 : 0.015358921140432358
Loss at batch 40 : 0.009362081065773964
Loss at batch 50 : 0.01535165123641491
Loss at batch 60 : 0.017279215157032013
Loss at batch 70 : 0.011104011908173561
Loss at batch 80 : 0.0103292940184474
Loss at batch 90 : 0.020360587164759636
Loss at batch 100 : 0.007459742482751608
Loss at batch 110 : 0.012568679638206959
Loss at batch 120 : 0.011372477747499943
Loss at batch 130 : 0.012115377932786942
Loss at batch 140 : 0.01121642254292965
Loss at batch 150 : 0.017897270619869232
Loss at batch 160 : 0.009301348589360714
Loss at batch 170 : 0.007059963885694742
Loss at batch 180 : 0.0252898707985878
Loss at batch 190 : 0.02056136354804039
Loss at batch 200 : 0.027803342789411545
Loss at batch 210 : 0.025767747312784195
Loss at batch 220 : 0.014586352743208408
Loss at batch 230 : 0.01630416326224804
Loss at batch 240 : 0.005410179030150175
Loss at batch 250 : 0.024498537182807922
Loss at batch 260 : 0.022281430661678314
Loss at batch 270 : 0.01208143774420023
Loss at batch 280 : 0.01729278452694416
Loss at batch 290 : 0.011302053928375244
Loss at batch 300 : 0.008257756009697914
Loss at batch 310 : 0.018236136063933372
Loss at batch 320 : 0.011546896770596504
Loss at batch 330 : 0.0110523896291852
Loss at batch 340 : 0.020091839134693146
Loss at batch 350 : 0.021570414304733276
Loss at batch 360 : 0.008728607557713985
Loss at batch 370 : 0.01885141059756279
epoch87 finished!
Loss at batch 10 : 0.018224071711301804
Loss at batch 20 : 0.012735225260257721
Loss at batch 30 : 0.0064633251167833805
Loss at batch 40 : 0.01969229057431221
Loss at batch 50 : 0.021323468536138535
Loss at batch 60 : 0.026525570079684258
Loss at batch 70 : 0.009211275726556778
Loss at batch 80 : 0.014231317676603794
Loss at batch 90 : 0.015295061282813549
Loss at batch 100 : 0.020614081993699074
Loss at batch 110 : 0.013607747852802277
Loss at batch 120 : 0.023150673136115074
Loss at batch 130 : 0.01924598589539528
Loss at batch 140 : 0.009900718927383423
Loss at batch 150 : 0.014363040216267109
Loss at batch 160 : 0.007755427621304989
Loss at batch 170 : 0.012414129450917244
Loss at batch 180 : 0.011962974444031715
Loss at batch 190 : 0.023359356448054314
Loss at batch 200 : 0.02095063030719757
Loss at batch 210 : 0.01054958626627922
Loss at batch 220 : 0.029307641088962555
Loss at batch 230 : 0.022951386868953705
Loss at batch 240 : 0.015300653874874115
Loss at batch 250 : 0.016759026795625687
Loss at batch 260 : 0.01368950866162777
Loss at batch 270 : 0.007179693318903446
Loss at batch 280 : 0.013944114558398724
Loss at batch 290 : 0.011259808205068111
Loss at batch 300 : 0.017835333943367004
Loss at batch 310 : 0.0116836316883564
Loss at batch 320 : 0.014940558932721615
Loss at batch 330 : 0.015370204113423824
Loss at batch 340 : 0.017932439222931862
Loss at batch 350 : 0.014378952793776989
Loss at batch 360 : 0.017640693113207817
Loss at batch 370 : 0.013765204697847366
epoch88 finished!
Loss at batch 10 : 0.019147120416164398
Loss at batch 20 : 0.029957352206110954
Loss at batch 30 : 0.01004374772310257
Traceback (most recent call last):
  File "train3.py", line 89, in <module>
    train(config_para)
  File "train3.py", line 69, in train
    torch.nn.utils.clip_grad_norm_(dehaze_net.parameters(), config.grad_clip_norm)
  File "/home/ytchen/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/nn/utils/clip_grad.py", line 38, in clip_grad_norm_
    if clip_coef < 1:
  File "/home/ytchen/anaconda3/envs/pytorch/lib/python3.6/site-packages/torch/utils/data/_utils/signal_handling.py", line 66, in handler
    _error_if_any_worker_fails()
RuntimeError: DataLoader worker (pid 13425) is killed by signal: Terminated. 
